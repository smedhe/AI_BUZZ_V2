{
  "page-1": {
    "title": "Introduction to Efficient Transformers",
    "section": "Overview",
    "markdown": "# Introduction to Efficient Transformers\n## Overview\nThe Efficient Transformers library, located in the `QEfficient/transformers` directory, is a collection of optimized transformer models that improve efficiency and support various features such as speculative decoding, finite adapters, and quantized models. These features are implemented in files such as `QEfficient/transformers/models/modeling_auto.py` and `QEfficient/transformers/transform.py`.\n\n## Key Components / Concepts\nThe library takes in input models and outputs compiled models with improved performance, and it supports multiple sequence lengths and custom pooling configurations. Key side-effects of using this library include improved inference speed and reduced memory usage, making it suitable for large-scale natural language processing tasks. The main components are defined in `QEfficient/transformers/models/__init__.py`.\n\n## How it Works\nThe library provides support for various models, including Llama, Gemma, and Granite, and enables features like sentence embedding and gradient checkpointing. It also supports QNN compilation for AutoModel classes, as seen in `QEfficient/transformers/models/modeling_auto.py`.\n\n## Example(s)\nTo use the Efficient Transformers library, you can initialize a model using the `from_pretrained` method and then compile it for Cloud AI 100 using the `compile` method. For more information, refer to the `README.md` file.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] -->|Optimize|> B[Optimized Model]\n    B -->|Compile|> C[Compiled Model]\n    C -->|Deploy|> D[Cloud AI 100]\n```\nThis flowchart shows the process of optimizing and compiling a model for Cloud AI 100 using the Efficient Transformers library.\n\n## References\n* `[README.md](README.md)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/transformers/models/__init__.py](QEfficient/transformers/models/__init__.py)`"
  },
  "page-4": {
    "title": "Introduction to Tests",
    "section": "Overview",
    "markdown": "# Introduction to Tests\n## Overview\nThe Efficient Transformers repository, located at `Efficient Transformers Wiki`, contains a comprehensive set of tests to ensure the correctness and performance of its models and functionalities, as outlined in [tests/README.md](tests/README.md). These tests cover various aspects, including inference, export, compilation, and execution on different platforms, such as those described in [docs/index.md](docs/index.md).\n\n## Key Components / Concepts\nThe tests are designed to validate the performance of different models, such as PyTorch, ONNX, and Cloud AI 100, under various conditions, including continuous batching and quantization, as implemented in [tests/transformers/models/test_causal_lm_models.py](tests/transformers/models/test_causal_lm_models.py). The tests also cover speculative decoding and semantic search.\n\n## How it Works\nThe tests are implemented using the Pytest framework and can be run using the `pytest` command, as configured in [scripts/Jenkinsfile](scripts/Jenkinsfile). The tests create temporary files and directories, which can be cleaned up using provided fixtures or scripts.\n\n## Example(s)\nFor example, the `test_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100` function validates the performance of different models, including PyTorch, PyTorch with KV changes, ONNX, and Cloud AI 100, for a specific prompt length.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Tests] --> B[Inference]\n    A --> C[Export]\n    A --> D[Compilation]\n    A --> E[Execution]\n    B --> F[PyTorch]\n    B --> G[ONNX]\n    B --> H[Cloud AI 100]\n    C --> I[Model Export]\n    D --> J[Model Compilation]\n    E --> K[Model Execution]\n```\nCaption: Overview of the tests and their components.\n\n## References\n* `[tests/transformers/models/test_causal_lm_models.py](tests/transformers/models/test_causal_lm_models.py)`\n* `[tests/README.md](tests/README.md)`\n* `[docs/index.md](docs/index.md)`\n* `[scripts/Jenkinsfile](scripts/Jenkinsfile)`"
  },
  "page-11": {
    "title": "Introduction to QEfficient",
    "section": "Overview",
    "markdown": "# Introduction to QEfficient\n## Overview\nQEfficient is a Python library designed for efficient inference of Hugging Face Transformer models on Qualcomm Cloud AI 100. It provides a framework for developers to work with transformer models, supporting various features such as model quantization, caching, and data serialization.\n\n## Key Components / Concepts\nThe library consists of several key components, including:\n* `QEFFAutoModelForCausalLM`: a class for manipulating causal language models from the Hugging Face hub\n* `QEFFAutoModelForImageTextToText`: a class for working with multimodal language models from the Hugging Face hub\n* `from_pretrained`: a method for initializing QEfficient models from pre-trained models\n\n## How it Works\nThe library uses the `from_pretrained` method to initialize models from pre-trained models. It then provides various methods for exporting, compiling, and generating text using the initialized models.\n\n## Example(s)\nAn example of using the `QEFFAutoModelForCausalLM` class can be found in the `QEfficient/transformers/models/modeling_auto.py` file.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Initialize Model] --> B[Export Model]\n    B --> C[Compile Model]\n    C --> D[Generate Text]\n```\nThis flowchart illustrates the basic workflow of using QEfficient to initialize a model, export it, compile it, and generate text using the compiled model.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/generation/text_generation_inference.py`\n* `QEfficient/utils/__init__.py`"
  },
  "page-21": {
    "title": "Introduction to Efficient Transformers",
    "section": "Overview",
    "markdown": "# Introduction to Efficient Transformers\n## Overview\nThe Efficient Transformers library, located in the `QEfficient` directory, is a collection of optimized transformer models designed for efficient inference on Qualcomm Cloud AI 100. It provides support for various features such as speculative decoding, finite adapters, and quantized models, as described in [README.md](README.md).\n\n## Key Components / Concepts\nThe library includes several key components, including:\n* `QEFFAutoModel`: a class for manipulating transformer models from the Hugging Face hub, defined in [QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)\n* `QEFFTransformersBase`: a parent class for models provided by QEFF from transformers, outlined in [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)\n* `QEFFBaseModel`: a base class for all model classes, providing utility methods for child classes, as specified in [pyproject.toml](pyproject.toml)\n\n## How it Works\nThe library works by taking input models and outputting compiled models with improved performance, as configured in [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py). It supports multiple sequence lengths and custom pooling configurations.\n\n## Example(s)\nTo use the library, you can initialize a model using the `from_pretrained` method, similar to transformers.AutoModel. For example:\n```python\nfrom QEfficient import QEFFAutoModel\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n```\nYou can then compile the model for Cloud AI 100 using the `compile` method:\n```python\nmodel.compile(num_cores=16)\n```\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] -->|from_pretrained|> B[QEFFAutoModel]\n    B -->|compile|> C[Compiled Model]\n    C -->|generate|> D[Output]\n```\nThis flowchart shows the basic workflow of the Efficient Transformers library, from input model to output.\n\n## References\n* `[README.md](README.md)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[pyproject.toml](pyproject.toml)`"
  },
  "page-24": {
    "title": "Introduction to Efficient Transformers",
    "section": "Overview",
    "markdown": "# Introduction to Efficient Transformers\n## Overview\nThe Efficient Transformers library, located at `QEfficient/transformers`, is a collection of optimized transformer models designed to improve efficiency and support various features such as speculative decoding, finite adapters, and quantized models. The library's core functionality is outlined in [README.md](README.md).\n\n## Key Components / Concepts\nThe library takes in input models and outputs compiled models with improved performance, supporting multiple sequence lengths and custom pooling configurations, as defined in [QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py). Key side-effects of using this library include improved inference speed and reduced memory usage, making it suitable for large-scale natural language processing tasks.\n\n## How it Works\nThe library provides support for various models, including Llama, Gemma, and Granite, and enables features like sentence embedding and gradient checkpointing. It also supports QNN compilation for AutoModel classes and implements post-attention hidden size projections to speculate tokens ahead of the base model, as implemented in [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py).\n\n## Example(s)\nTo use the Efficient Transformers library, you can initialize a model using the `from_pretrained` method and then compile it for Cloud AI 100 using the `compile` method. For more information, refer to [docs/source/introduction.md](docs/source/introduction.md).\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] -->|Optimization|> B[Compiled Model]\n    B -->|Inference|> C[Output]\n    C -->|Evaluation|> D[Performance Metrics]\n```\nCaption: Overview of the Efficient Transformers library workflow.\n\n## References\n* `[README.md](README.md)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[docs/source/introduction.md](docs/source/introduction.md)`"
  },
  "page-29": {
    "title": "Introduction to Scripts",
    "section": "Overview",
    "markdown": "# Introduction to Scripts\n## Overview\nThe Efficient Transformers repository, located at `EfficientTransformers/`, provides a variety of scripts to support different tasks, such as inference, perplexity computation, and model integration. These scripts are designed to work with the Cloud AI 100 hardware and utilize the QEfficient library, which can be found at `QEfficient/`.\n\n## Key Components / Concepts\nThe key components of the scripts include the `cloud_ai_100_exec_kv` function, defined in `QEfficient/generation/text_generation_inference.py`, which generates output until a specified length or end-of-sentence token is reached. The `get_compilation_dims` function, used to fetch compilation dimensions from a `specializations.json` file, is crucial for model integration.\n\n## How it Works\nThe scripts work by executing a compiled QPC on Cloud AI 100 hardware cards. The execution is sequential, based on the batch size of the compiled model and the number of prompts passed, as demonstrated in `examples/intern_example/internvl_inference.py`.\n\n## Example(s)\nAn example of how to use the scripts can be found in the `examples/intern_example/` directory, which contains a script for running inference on the InternVL-1B model. This example illustrates the usage of the `cloud_ai_100_exec_kv` function for generating output.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Script Execution] --> B[QPC Compilation]\n    B --> C[Cloud AI 100 Execution]\n    C --> D[Output Generation]\n    D --> E[Perplexity Computation]\n```\nCaption: Script Execution Flowchart\n\n## References\n* `scripts/perplexity_computation/calculate_perplexity.py`\n* `examples/intern_example/internvl_inference.py`\n* `QEfficient/generation/text_generation_inference.py`"
  },
  "page-2": {
    "title": "Docker Containerization",
    "section": "Deployment/Infrastructure",
    "markdown": "# Docker Containerization\n## Overview\nDocker containerization is a key aspect of deploying the Efficient Transformers library, allowing for efficient and reproducible deployment on various environments, including Qualcomm Cloud AI 100, as described in [docs/source/introduction.md](docs/source/introduction.md).\n\n## Key Components / Concepts\nThe Docker containerization process involves several key components, including:\n* Dockerfile: a text file that contains instructions for building a Docker image, such as the one found in [Dockerfile](Dockerfile)\n* Docker image: a lightweight and standalone executable package that includes everything needed to run an application, like the Efficient Transformers library\n* Docker container: a runtime instance of a Docker image, which can be used to execute applications like [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)\n\n## How it Works\nThe Docker containerization process works as follows:\n1. A Dockerfile is created, which includes instructions for building a Docker image.\n2. The Docker image is built using the instructions in the Dockerfile.\n3. The Docker image is then used to create a Docker container.\n4. The Docker container is run, which executes the application inside the container, such as exporting models using [QEfficient/exporter/export_hf_to_cloud_ai_100.py](QEfficient/exporter/export_hf_to_cloud_ai_100.py).\n\n## Example(s)\nAn example of a Dockerfile for the Efficient Transformers library can be found in the [Dockerfile](Dockerfile) file, which demonstrates how to build a Docker image for the library.\n\n## Diagram(s)\n```mermaid\ngraph LR\n    A[Dockerfile] -->|Build|> B[Docker Image]\n    B -->|Create|> C[Docker Container]\n    C -->|Run|> D[Application]\n```\nCaption: Docker Containerization Process\n\n## References\n* [Dockerfile](Dockerfile)\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/exporter/export_hf_to_cloud_ai_100.py](QEfficient/exporter/export_hf_to_cloud_ai_100.py)`\n* `[docs/source/introduction.md](docs/source/introduction.md)`"
  },
  "page-9": {
    "title": "Testing Cloud Export and Inference",
    "section": "Deployment/Infrastructure",
    "markdown": "# Testing Cloud Export and Inference\n## Overview\nTesting cloud export and inference is a crucial step in ensuring the efficient deployment of models on cloud infrastructure. This process involves exporting the model to a compatible format, compiling it for the target hardware, and executing it to generate predictions.\n\n## Key Components / Concepts\nThe key components involved in testing cloud export and inference include:\n- Model export: Converting the model into a format compatible with the cloud infrastructure, such as ONNX, which is supported by the `QEfficient/cloud/execute.py` file.\n- Model compilation: Compiling the exported model for the target hardware to optimize performance, as seen in the `tests/cloud/test_export_compile_execute.py` file.\n- Model execution: Executing the compiled model to generate predictions, which can be used for text generation inference as shown in `QEfficient/generation/text_generation_inference.py`.\n\n## How it Works\nThe process of testing cloud export and inference works as follows:\n1. **Model Export**: The model is exported to a compatible format such as ONNX using the `tests/cloud/test_export_compile_execute.py` file.\n2. **Model Compilation**: The exported model is compiled for the target cloud hardware, utilizing the compilation functionality provided in `QEfficient/cloud/execute.py`.\n3. **Model Execution**: The compiled model is executed on the cloud infrastructure to generate predictions, which can then be used for various applications such as text generation.\n\n## Example(s)\nAn example of testing cloud export and inference can be seen in the `tests/cloud/test_export_compile_execute.py` file, which tests the export, compilation, and execution of a model. Additionally, the `examples/multiprojs_spd_inference.py` file demonstrates how to use the exported and compiled model for inference.\n\n## Diagram(s)\n```mermaid\ngraph TD\n    A[Model Export] --> B[Model Compilation]\n    B --> C[Model Execution]\n    C --> D[Prediction Generation]\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style B fill:#f9f,stroke:#333,stroke-width:4px\n    style C fill:#f9f,stroke:#333,stroke-width:4px\n    style D fill:#f9f,stroke:#333,stroke-width:4px\n```\nCaption: Cloud Export and Inference Process\n\n## References\n- `tests/cloud/test_export_compile_execute.py`\n- `QEfficient/cloud/execute.py`\n- `QEfficient/generation/text_generation_inference.py`\n- `examples/multiprojs_spd_inference.py`"
  },
  "page-15": {
    "title": "Cloud Deployment",
    "section": "Deployment/Infrastructure",
    "markdown": "# Cloud Deployment\n## Overview\nCloud deployment is a crucial aspect of Efficient Transformers, allowing users to leverage the power of cloud computing for their transformer-based applications. This section provides an overview of the cloud deployment process using QEfficient, which is located in the `Efficient Transformers Wiki/Deployment/Infrastructure` directory.\n\n## Key Components / Concepts\nThe key components involved in cloud deployment are:\n- QEfficient: The framework used for optimizing and deploying transformer models on cloud infrastructure, as described in `QEfficient/cloud/README.md`.\n- Cloud AI 100: The cloud platform used for deploying and running the optimized models, with configuration details in `QEfficient/cloud/config.json`.\n- QPC (Quantized Processing Code): The compiled code generated by QEfficient for running on Cloud AI 100 hardware, as outlined in `QEfficient/exporter/export_hf_to_cloud_ai_100.py`.\n\n## How it Works\nThe cloud deployment process involves the following steps:\n1. Model Optimization: The transformer model is optimized using QEfficient's `transform` function, which replaces the torch.nn.Module layers with optimized implementations, as shown in `QEfficient/transformer/optimization.py`.\n2. Model Export: The optimized model is exported to an ONNX graph using QEfficient's `export` function, with example usage in `QEfficient/exporter/export_hf_to_cloud_ai_100.py`.\n3. Compilation: The ONNX graph is compiled into QPC code using QEfficient's `compile` function, as demonstrated in `QEfficient/cloud/compile.py`.\n4. Deployment: The QPC code is deployed on Cloud AI 100 infrastructure, with deployment scripts located in `QEfficient/cloud/deploy`.\n5. Execution: The deployed model is executed on Cloud AI 100 hardware using QEfficient's `execute` function, as seen in `QEfficient/cloud/execute.py`.\n\n## Example(s)\nAn example of cloud deployment can be seen in the `QEfficient/cloud/execute.py` file, which demonstrates how to execute a model on Cloud AI 100 using the `main` function. This example showcases the entire deployment process, from model optimization to execution.\n\n## Diagram(s)\n```mermaid\ngraph LR\n    A[Model Optimization] --> B[Model Export]\n    B --> C[Compilation]\n    C --> D[Deployment]\n    D --> E[Execution]\n    style A fill:#f9f,stroke:#333,stroke-width:4px\n    style B fill:#f9f,stroke:#333,stroke-width:4px\n    style C fill:#f9f,stroke:#333,stroke-width:4px\n    style D fill:#f9f,stroke:#333,stroke-width:4px\n    style E fill:#f9f,stroke:#333,stroke-width:4px\n```\nCloud Deployment Process\n\n## References\n- `QEfficient/cloud/execute.py`\n- `QEfficient/exporter/export_hf_to_cloud_ai_100.py`\n- `QEfficient/cloud/infer.py`\n- `QEfficient/transformer/optimization.py`"
  },
  "page-28": {
    "title": "Requirements and Installation",
    "section": "Deployment/Infrastructure",
    "markdown": "# Requirements and Installation\n## Overview\nThe Efficient Transformers library is a Python interface for Hugging Face Transformer models, designed for efficient inference on Qualcomm Cloud AI 100. To use this library, you need to have Python 3.8 or later installed on your system, as specified in the [pyproject.toml](pyproject.toml) file.\n\n## Key Components / Concepts\nThe library has several key dependencies, including transformers, torch, and onnx, as outlined in the [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py) and [QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py) files. It is compatible with Python versions 3.8 to 3.10.\n\n## How it Works\nTo install the library, you can use pip to install the required packages, as described in the [README.md](README.md) file. You can also use a Docker image to set up and install the library.\n\n## Example(s)\nYou can install the library using pip:\n```bash\npip install git+https://github.com/quic/efficient-transformers\n```\nAlternatively, you can use a Docker image to set up and install the library:\n```bash\ndocker build -t qefficient-library .\ndocker run -it qefficient-library\n```\n\n## Diagram(s)\n```mermaid\ngraph LR\n    A[Python 3.8+] -->|install|> B[pip]\n    B -->|install|> C[QEfficient library]\n    C -->|import|> D[Transformer models]\n    D -->|use|> E[Qualcomm Cloud AI 100]\n```\nCaption: Installation and usage of the QEfficient library.\n\n## References\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[pyproject.toml](pyproject.toml)`\n* `[README.md](README.md)`"
  },
  "page-33": {
    "title": "Jenkinsfile",
    "section": "Deployment/Infrastructure",
    "markdown": "# Jenkinsfile\n## Overview\nThe Jenkinsfile is a crucial component in the deployment and infrastructure of the Efficient Transformers project, located in the `Efficient Transformers Wiki` repository under `Deployment/Infrastructure`. It is used to define the pipeline for building, testing, and deploying the project.\n\n## Key Components / Concepts\nThe Jenkinsfile is written in Groovy and utilizes the Jenkins pipeline syntax. It consists of several stages, including installation, testing, and deployment, which are defined in the `scripts/Jenkinsfile` file.\n\n## How it Works\nThe Jenkinsfile works by defining a series of stages that are executed in sequence. Each stage performs a specific task, such as installing dependencies, running tests, or deploying the project. The pipeline syntax is used to define these stages and the order in which they are executed.\n\n## Example(s)\nAn example of a Jenkinsfile can be seen in the `scripts/Jenkinsfile` file, which defines a pipeline for building and testing the Efficient Transformers project. This file demonstrates how to use the Jenkins pipeline syntax to define a pipeline that installs dependencies, runs tests, and deploys the project.\n\n## Diagram(s)\n```mermaid\ngraph TD\n    A[Install Dependencies] --> B[Run Tests]\n    B --> C[Deploy Project]\n    C --> D[Verify Deployment]\n```\nThis diagram shows the basic flow of the Jenkinsfile pipeline, from installation to deployment.\n\n## References\n* `scripts/Jenkinsfile`\n* `docs/source/cli_api.md`\n* `QEfficient/generation/text_generation_inference.py`\n* `examples/multiprojs_spd_inference.py`"
  },
  "page-3": {
    "title": "Library Configuration",
    "section": "Core Features",
    "markdown": "# Library Configuration\n## Overview\nThe Efficient Transformers library can be configured using the `pyproject.toml` file, located in the root directory of the project. This file allows users to specify various settings and options for the library, including model architecture, quantization, and optimization settings.\n\n## Key Components / Concepts\nThe library configuration involves several key components, including:\n* Model settings: Users can configure model-specific settings, such as the model architecture and hyperparameters, by modifying the `model_architecture` and `hyperparameters` fields in the `pyproject.toml` file.\n* Quantization settings: The library provides options for quantizing models, which can improve performance and reduce memory usage, through the `quantization` field in the `pyproject.toml` file.\n* Optimization settings: Users can configure optimization settings, such as the optimizer and learning rate schedule, by modifying the `optimizer` and `learning_rate_schedule` fields in the `pyproject.toml` file.\n\n## How it Works\nThe library configuration works by reading the settings from the `pyproject.toml` file, located at `/path/to/project/pyproject.toml`, and applying them to the model and optimization pipeline. The settings are used to initialize the model, optimizer, and other components of the library, as defined in `QEfficient/transformers/models/modeling_auto.py`.\n\n## Example(s)\nFor example, to configure the library to use a specific model architecture, users can add the following lines to their `pyproject.toml` file:\n```toml\n[tool.qefficient]\nmodel_architecture = \"transformer\"\n```\nThis configuration will be applied when initializing the model, as defined in `QEfficient/transformers/transform.py`.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[pyproject.toml] -->|read settings|> B[Library Configuration]\n    B -->|apply settings|> C[Model Initialization]\n    C -->|initialize model|> D[Optimization Pipeline]\n    D -->|optimize model|> E[Training]\n```\nConfiguration Flowchart\n\n## References\n* `/path/to/project/pyproject.toml`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/utils/_utils.py`"
  },
  "page-5": {
    "title": "Testing Causal LM Models",
    "section": "Core Features",
    "markdown": "# Testing Causal LM Models\n## Overview\nTesting causal language models is crucial for ensuring their performance and accuracy. This page provides an overview of testing causal LM models using PyTorch and KV models, with a focus on the implementation details found in `tests/transformers/models/test_causal_lm_models.py` and `tests/transformers/test_causal_lm.py`.\n\n## Key Components / Concepts\nThe key components involved in testing causal LM models include:\n* PyTorch models\n* KV models\n* QNN (Quantization Neural Network) configuration\n* ONNX models\n* Cloud AI 100 models\nThese components are utilized in the testing process to validate the consistency of outputs from different models.\n\n## How it Works\nThe testing process involves loading a causal language model, transforming it into a KV model, and then comparing its performance with other models such as ONNX and Cloud AI 100 models. The `check_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100` function, defined in `tests/transformers/models/test_causal_lm_models.py`, is used to validate the consistency of outputs from different models. This function takes into account the model configurations specified in `tests/transformers/models/custom_tiny_model_configs.json`.\n\n## Example(s)\nAn example of testing a causal LM model can be seen in the `test_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100` function, which takes a model name as input and validates the performance of different models. This example demonstrates how to utilize the testing framework to compare the performance of various models.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Causal LM Model] --> B[Transform to KV Model]\n    B --> C[Compare with ONNX Model]\n    C --> D[Compare with Cloud AI 100 Model]\n    D --> E[Validate Consistency of Outputs]\n```\nThis flowchart illustrates the process of testing a causal LM model, from loading the model to validating the consistency of outputs from different models.\n\n## References\n* `tests/transformers/models/test_causal_lm_models.py`\n* `tests/transformers/test_causal_lm.py`\n* `tests/transformers/models/custom_tiny_model_configs.json`\n* `docs/source/core_features/testing_causal_lm_models.md`"
  },
  "page-6": {
    "title": "Testing Speech Seq2Seq Models",
    "section": "Core Features",
    "markdown": "# Testing Speech Seq2Seq Models\n## Overview\nTesting speech seq2seq models is crucial for ensuring their performance and accuracy in various applications. This section provides an overview of the testing process using PyTorch and KV models, which can be found in the `tests/transformers/models/test_speech_seq2seq_models.py` and `QEfficient/transformers/models/modeling_auto.py` files.\n\n## Key Components / Concepts\nThe key components involved in testing speech seq2seq models include:\n* PyTorch models\n* KV models\n* `QEFFAutoModelForSpeechSeq2Seq` class\n* `run_seq2seq_pytorch_hf` function\n* `run_seq2seq_pytorch_with_kv` function\n* `run_seq2seq_ort` function\n\n## How it Works\nThe testing process involves the following steps:\n1. Load the pre-trained model using the `QEFFAutoModelForSpeechSeq2Seq` class.\n2. Prepare the input audio data and sample rate.\n3. Run the PyTorch inference on the model using the `run_seq2seq_pytorch_hf` function.\n4. Run the PyTorch inference with KV changes using the `run_seq2seq_pytorch_with_kv` function.\n5. Run the onnxruntime inference on the model using the `run_seq2seq_ort` function.\n6. Compare the output features generated by each model.\n\n## Example(s)\nAn example of testing a speech seq2seq model using PyTorch and KV models can be seen in the `test_seq2seq_pytorch_vs_kv_vs_ort_vs_ai100` function, which is defined in the `tests/transformers/test_speech_seq2seq.py` file.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Load Pre-trained Model] --> B[Prepare Input Audio Data]\n    B --> C[Run PyTorch Inference]\n    C --> D[Run PyTorch Inference with KV Changes]\n    D --> E[Run Onnxruntime Inference]\n    E --> F[Compare Output Features]\n```\nThis flowchart illustrates the steps involved in testing a speech seq2seq model using PyTorch and KV models.\n\n## References\n* `tests/transformers/models/test_speech_seq2seq_models.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `tests/transformers/test_speech_seq2seq.py`\n* `QEfficient/utils/_utils.py`"
  },
  "page-12": {
    "title": "Modeling Utilities",
    "section": "Core Features",
    "markdown": "# Modeling Utilities\n## Overview\nThe QEfficient library provides various modeling utilities to support efficient transformer models, including custom attention mechanisms and rotary embedding implementations, which can be found in `QEfficient/transformers/models/modeling_auto.py` and `QEfficient/transformers/transform.py`.\n\n## Key Components / Concepts\nThe key components of the modeling utilities are:\n* Custom attention mechanisms, such as those defined in `QEfficient/transformers/models/modeling_auto.py`\n* Rotary embedding implementations\n* Model transformation and compilation for Cloud AI 100, utilizing `QEfficient/base/modeling_qeff.py`\n\n## How it Works\nThe modeling utilities work by providing a set of functions and classes that can be used to create and manipulate efficient transformer models. The `QEFFAutoModel` class, located in `QEfficient/transformers/models/modeling_auto.py`, is the base class for all models, and it provides methods for transforming and compiling models for Cloud AI 100.\n\n## Example(s)\nAn example of using the modeling utilities is:\n```python\nfrom QEfficient import QEFFAutoModel\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\nmodel.compile(num_cores=16)\n```\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Transform Model]\n    B --> C[Compile Model]\n    C --> D[Run Model]\n```\nModeling Utilities Flowchart\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/base/modeling_qeff.py`\n* `QEfficient/README.md`"
  },
  "page-13": {
    "title": "Quantization and Compression",
    "section": "Core Features",
    "markdown": "# Quantization and Compression\n## Overview\nQuantization and compression are essential techniques in optimizing transformer models for efficient deployment. QEfficient supports various quantization methods, including FP8 and AWQ, to reduce the memory footprint and computational requirements of models.\n\n## Key Components / Concepts\nThe key components involved in quantization and compression include:\n- Quantization methods: FP8, AWQ, and GPTQ\n- Quantizer classes: `QEffExtendedQuantizationMethod` and `QEffAutoQuantizationMethod`\n- Model optimization: `transform_lm` function for replacing torch.nn.Module layers with optimized QEff layers\n\n## How it Works\nThe quantization and compression process involves the following steps:\n1. Model initialization: Initialize a QEfficient model using the `from_pretrained` method.\n2. Quantization: Apply quantization to the model using the `transform_lm` function.\n3. Compression: Compress the quantized model using techniques such as tensor compression.\n\n## Example(s)\n```python\nfrom QEfficient import QEFFAutoModel\n\n# Initialize a QEfficient model\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Apply quantization to the model\nmodel = transform_lm(model)\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Model Initialization] --> B[Quantization]\n    B --> C[Compression]\n    C --> D[Optimized Model]\n```\nQuantization and Compression Flowchart\n\n## References\n- `QEfficient/transformers/quantizers/quantizer_compressed_tensors.py`\n- `QEfficient/transformers/transform.py`\n- `QEfficient/transformers/models/modeling_auto.py` \n- `QEfficient/utils/_utils.py`"
  },
  "page-19": {
    "title": "Speculative Decoding",
    "section": "Core Features",
    "markdown": "# Speculative Decoding\n## Overview\nSpeculative decoding is a technique used to improve the performance of language models by generating multiple possible outputs and selecting the most likely one. This approach can be particularly useful in applications where the model needs to generate text quickly and efficiently, as seen in the `examples/draft_spd_inference.py` file.\n\n## Key Components / Concepts\nThe key components of speculative decoding include:\n* **Speculative tokens**: These are the possible output tokens that the model generates at each step, based on the input sequence and context.\n* **Prefill sequence length**: This is the length of the input sequence that the model uses to generate the speculative tokens, which is defined in the `QEfficient/transformers/models/pytorch_transforms.py` file.\n* **Context length**: This is the length of the context that the model uses to generate the speculative tokens.\n* **Prefill batch size**: This is the batch size of the input sequence that the model uses to generate the speculative tokens.\n\n## How it Works\nThe speculative decoding process works as follows:\n1. The model generates a set of speculative tokens based on the input sequence and context.\n2. The model computes the likelihood of each speculative token and selects the most likely one.\n3. The model generates the next token in the sequence based on the selected speculative token.\n4. The process is repeated until the desired output sequence is generated, as demonstrated in the `examples/multiprojs_spd_inference.py` file.\n\n## Example(s)\nAn example of speculative decoding can be seen in the `draft_spec_decode_inference` function in the `examples/draft_spd_inference.py` file. This function takes in a list of prompts, a draft model name, a target model name, and other parameters, and returns the generated text and performance metrics. Another example is the `pld_spec_decode_inference` function in the `examples/pld_spd_inference.py` file.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Input Sequence] --> B[Speculative Token Generation]\n    B --> C[Token Likelihood Computation]\n    C --> D[Token Selection]\n    D --> E[Next Token Generation]\n    E --> F[Output Sequence]\n```\nThis diagram shows the speculative decoding process, from input sequence to output sequence.\n\n## References\n* `examples/draft_spd_inference.py`\n* `examples/multiprojs_spd_inference.py`\n* `QEfficient/transformers/models/pytorch_transforms.py`\n* `examples/pld_spd_inference.py`"
  },
  "page-25": {
    "title": "Python API Reference",
    "section": "Core Features",
    "markdown": "# Python API Reference\n## Overview\nThe Efficient Transformers library provides a Python API for working with transformer models, located in `QEfficient/transformers/models/`. The API is designed to be similar to the Hugging Face Transformers library, with some additional features and optimizations for efficient inference, as outlined in `QEfficient/transformers/transform.py`.\n\n## Key Components / Concepts\nThe key components of the Python API are:\n* `QEFFAutoModel`: a class for working with transformer models from the Hugging Face hub, defined in `QEfficient/transformers/models/modeling_auto.py`.\n* `QEFFAutoModelForCausalLM`: a class for working with causal language models from the Hugging Face hub, built upon `QEfficient/transformers/models/modeling_auto.py`.\n* `transform`: a function for optimizing transformer models for Cloud AI 100, implemented in `QEfficient/transformers/transform.py`.\n\n## How it Works\nThe API works by providing a set of classes and functions that can be used to load, optimize, and run transformer models. The `QEFFAutoModel` and `QEFFAutoModelForCausalLM` classes provide a convenient interface for working with transformer models, while the `transform` function provides a way to optimize models for Cloud AI 100, utilizing `QEfficient/transformers/models/pytorch_transforms.py` for PyTorch transformations.\n\n## Example(s)\nHere is an example of how to use the `QEFFAutoModel` class to load a transformer model and optimize it for Cloud AI 100:\n```python\nfrom QEfficient import QEFFAutoModel\n\n# Load a transformer model\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Optimize the model for Cloud AI 100\nmodel.transform(form_factor=\"cloud\")\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Optimize Model]\n    B --> C[Run Model]\n    C --> D[Get Results]\n```\nThis diagram shows the basic workflow of the Python API, from loading a model to getting results.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/transformers/models/pytorch_transforms.py`"
  },
  "page-30": {
    "title": "Perplexity Computation",
    "section": "Core Features",
    "markdown": "# Perplexity Computation\n## Overview\nPerplexity computation is a crucial aspect of evaluating the performance of language models. It measures how well a model can predict a test set, given the model was trained on a training set.\n\n## Key Components / Concepts\nThe key components involved in perplexity computation include the language model, the test dataset, and the perplexity calculation script. The `calculate_perplexity.py` script, located at `scripts/perplexity_computation/calculate_perplexity.py`, is used to compute the perplexity of a given model on a specified dataset.\n\n## How it Works\nThe perplexity computation process involves the following steps:\n1. Load the test dataset and the language model using the `text_generation_inference.py` script from `QEfficient/generation/`.\n2. Prepare the input data for the model, including tokenization and formatting, as demonstrated in `examples/pld_spd_inference.py`.\n3. Use the `calculate_perplexity.py` script to compute the perplexity of the model on the test dataset.\n\n## Example(s)\nTo compute the perplexity of a model, you can use the `calculate_perplexity.py` script, providing the necessary arguments such as the model path, model type, dataset name, and context length. For instance, the `tests/transformers/spd/test_pld_inference.py` test case showcases how to utilize the `calculate_perplexity.py` script for perplexity computation.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Test Dataset] --> B[Load Language Model]\n    B --> C[Prepare Input Data]\n    C --> D[Compute Perplexity]\n    D --> E[Output Perplexity Score]\n```\nPerplexity Computation Flowchart\n\n## References\n* `scripts/perplexity_computation/calculate_perplexity.py`\n* `QEfficient/generation/text_generation_inference.py`\n* `examples/pld_spd_inference.py`\n* `tests/transformers/spd/test_pld_inference.py`"
  },
  "page-31": {
    "title": "Replicating KV Heads",
    "section": "Core Features",
    "markdown": "# Replicating KV Heads\n## Overview\nReplicating KV heads is a process used to optimize the performance of transformer models. This involves duplicating the key-value (KV) heads in the model, which can lead to improved efficiency and accuracy.\n\n## Key Components / Concepts\nThe key components involved in replicating KV heads include:\n* The `replicate_kv_heads` function, which takes in a model name, prompt, and repeat factor as inputs.\n* The `transform` function, which optimizes the model for Cloud AI 100 hardware.\n* The `from_pretrained` function, which loads a pre-trained model and prepares it for use.\n\n## How it Works\nThe process of replicating KV heads works as follows:\n1. The `replicate_kv_heads` function is called with the desired model name, prompt, and repeat factor.\n2. The function runs inference with the original model.\n3. The KV heads are replicated based on the specified repeat factor.\n4. Inference is run again with the modified model to validate the changes.\n5. The modified model is exported to ONNX format.\n\n## Example(s)\nAn example of replicating KV heads can be seen in the `scripts/replicate_kv_head/replicate_kv_heads.py` file.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Load Model] --> B[Run Inference]\n    B --> C[Replicate KV Heads]\n    C --> D[Run Inference Again]\n    D --> E[Export to ONNX]\n```\nThis flowchart illustrates the steps involved in replicating KV heads.\n\n## References\n* `scripts/replicate_kv_head/replicate_kv_heads.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `docs/core_features/replicating_kv_heads.md` \n* `README.md`"
  },
  "page-7": {
    "title": "Testing PEFT Models",
    "section": "Model Integration",
    "markdown": "# Testing PEFT Models\n## Overview\nTesting PEFT models involves validating their performance using various techniques, including Lora and adapter models, to ensure they function as expected and provide accurate results. This process is crucial for model integration and is facilitated by the `QEfficient` library, which provides the necessary tools and classes, such as `QEffAutoPeftModelForCausalLM`, for creating, exporting, compiling, and testing PEFT models.\n\n## Key Components / Concepts\nThe key components in testing PEFT models include:\n- **Lora Models**: These are used for efficient fine-tuning of large language models, as seen in `QEfficient/peft/lora/auto.py`.\n- **Adapter Models**: These are used to adapt pre-trained models to specific tasks, which is a critical step in the model creation process.\n\n## How it Works\nThe testing process involves the following steps:\n1. **Model Creation**: Create a PEFT model using the `create_peft_model` function from `QEfficient/peft/auto.py`, which combines a base model with an adapter.\n2. **Model Export**: Export the model using the `export` method of the `QEffAutoPeftModelForCausalLM` class.\n3. **Model Compilation**: Compile the exported model using the `compile` method, specifying parameters such as `prefill_seq_len` and `ctx_len`.\n4. **Model Testing**: Test the compiled model using various inputs and validate its performance, as demonstrated in `tests/peft/test_peft_model.py`.\n\n## Example(s)\n```python\nfrom QEfficient import QEffAutoPeftModelForCausalLM\n\n# Create a PEFT model\nmodel = QEffAutoPeftModelForCausalLM.from_pretrained(\"predibase/magicoder\", \"magicoder\")\n\n# Export the model\nmodel.export()\n\n# Compile the model\nmodel.compile(prefill_seq_len=32, ctx_len=1024)\n\n# Test the model\ninputs = ...  # A coding prompt\noutputs = model.generate(**inputs)\n```\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant Model as \"PEFT Model\"\n    participant Adapter as \"Adapter\"\n    participant Exporter as \"Exporter\"\n    participant Compiler as \"Compiler\"\n    participant Tester as \"Tester\"\n\n    Note over Model,Adapter: Create PEFT model\n    Model->>Adapter: Combine base model with adapter\n    Adapter->>Model: Return adapted model\n\n    Note over Model,Exporter: Export model\n    Model->>Exporter: Export model\n    Exporter->>Model: Return exported model\n\n    Note over Model,Compiler: Compile model\n    Model->>Compiler: Compile model\n    Compiler->>Model: Return compiled model\n\n    Note over Model,Tester: Test model\n    Model->>Tester: Test model\n    Tester->>Model: Return test results\n```\nCaption: Sequence diagram showing the testing process of PEFT models.\n\n## References\n- `tests/peft/test_peft_model.py`\n- `QEfficient/peft/auto.py`\n- `tests/peft/test_peft_onnx_transforms.py`\n- `QEfficient/peft/lora/auto.py`"
  },
  "page-8": {
    "title": "Testing Embedding Models",
    "section": "Model Integration",
    "markdown": "# Testing Embedding Models\n## Overview\nTesting embedding models is crucial for ensuring the accuracy and consistency of semantic search results. This process involves comparing the outputs of different models, including PyTorch, ONNX, and AI 100 runtime models, as seen in the `tests/transformers/models/test_embedding_models.py` file.\n\n## Key Components / Concepts\nThe key components in testing embedding models include the model itself, the input data, and the comparison function, such as `check_embed_pytorch_vs_ort_vs_ai100`, which is defined in `tests/transformers/models/test_embedding_models.py`. This function takes the model name, sequence length, and number of layers as inputs and returns the mean absolute difference (MAD) between the embeddings.\n\n## How it Works\nThe testing process works by first preparing the input data using a tokenizer, as shown in `tests/base/test_modeling_qeff.py`. Then, the original PyTorch model and the QEff transformed PyTorch model are used to generate outputs. The outputs are then compared using a pooling method, and the MAD is calculated.\n\n## Example(s)\nAn example of testing an embedding model can be seen in the `test_embed_model_pytorch_vs_onnx_vs_ai100` function, which takes a model as input and calls the `check_embed_pytorch_vs_ort_vs_ai100` function with specific parameters. This example is also referenced in `tests/transformers/models/test_causal_lm_models.py`.\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant PyTorch Model\n    participant QEff Model\n    participant Input Data\n    participant Comparison Function\n\n    Input Data->>PyTorch Model: Prepare input data\n    PyTorch Model->>Comparison Function: Generate output\n    QEff Model->>Comparison Function: Generate output\n    Comparison Function->>Comparison Function: Calculate MAD\n```\nThis sequence diagram shows the interaction between the PyTorch model, QEff model, input data, and comparison function.\n\n## References\n* `tests/transformers/models/test_embedding_models.py`\n* `tests/transformers/models/test_causal_lm_models.py`\n* `tests/base/test_modeling_qeff.py`\n* `QEfficient/peft/lora/auto.py`"
  },
  "page-14": {
    "title": "Model Implementations",
    "section": "Model Integration",
    "markdown": "# Model Implementations\n## Overview\nQEfficient provides various model implementations, including LLaMA, Gemma, and Whisper, which are designed to be efficient and scalable for a wide range of applications, as seen in the codebase at `QEfficient/transformers/models/modeling_auto.py`.\n\n## Key Components / Concepts\nThe key components of QEfficient's model implementations include:\n* **Model Architecture**: QEfficient's models are based on transformer architectures, which are well-suited for natural language processing tasks, as defined in `QEfficient/transformers/models/modeling_auto.py`.\n* **Efficient Inference**: QEfficient's models are optimized for efficient inference, making them suitable for deployment on a variety of hardware platforms, as implemented in `QEfficient/cloud/infer.py`.\n* **Scalability**: QEfficient's models are designed to be scalable, making them suitable for large-scale applications, as demonstrated in `QEfficient/transformers/models/gemma/modeling_gemma.py`.\n\n## How it Works\nQEfficient's model implementations work by leveraging the transformer architecture and optimizing it for efficient inference and scalability. The models are trained on large datasets and fine-tuned for specific tasks, making them highly accurate and effective, as shown in `QEfficient/transformers/models/llama/modeling_llama.py`.\n\n## Example(s)\nExamples of QEfficient's model implementations include:\n* **LLaMA**: A large language model that is highly accurate and efficient, as implemented in `QEfficient/transformers/models/llama/modeling_llama.py`.\n* **Gemma**: A model that is specifically designed for natural language processing tasks and is highly scalable, as defined in `QEfficient/transformers/models/gemma/modeling_gemma.py`.\n* **Whisper**: A model that is designed for speech recognition tasks and is highly accurate, as demonstrated in `QEfficient/transformers/models/whisper/modeling_whisper.py`.\n\n## Diagram(s)\n```mermaid\nclassDiagram\n    class Model {\n        +architecture: Transformer\n        +inference: Efficient\n        +scalability: High\n    }\n    class LLaMA {\n        +accuracy: High\n        +efficiency: High\n    }\n    class Gemma {\n        +scalability: High\n        +accuracy: High\n    }\n    class Whisper {\n        +accuracy: High\n        +efficiency: High\n    }\n    Model <|-- LLaMA\n    Model <|-- Gemma\n    Model <|-- Whisper\n```\nCaption: QEfficient's model implementations are based on the transformer architecture and are optimized for efficient inference and scalability.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/cloud/infer.py`\n* `QEfficient/transformers/models/gemma/modeling_gemma.py`\n* `QEfficient/transformers/models/llama/modeling_llama.py`"
  },
  "page-20": {
    "title": "Model Inference",
    "section": "Model Integration",
    "markdown": "# Model Inference\n## Overview\nModel inference is the process of using a trained model to make predictions on new, unseen data. In the context of QEfficient, model inference involves running the model on the Cloud AI 100 platform, as described in `QEfficient/cloud/infer.py`.\n\n## Key Components / Concepts\nThe key components involved in model inference are:\n* The trained model, which is compiled and exported to an ONNX file\n* The input data, processed and passed to the QPC file\n* The Cloud AI 100 platform, where the QPC file is executed\n\n## How it Works\nThe model inference process works as follows:\n1. The trained model is compiled and exported to an ONNX file, utilizing `QEfficient/transformers/models/modeling_auto.py`.\n2. The ONNX file is then compiled to a QPC file, which is optimized for the Cloud AI 100 platform.\n3. The input data is processed and passed to the QPC file, as demonstrated in `examples/granite_example/granite_vision_inference.py`.\n4. The QPC file is executed on the Cloud AI 100 platform, generating output.\n\n## Example(s)\nAn example of model inference can be seen in the `examples/granite_example/granite_vision_inference.py` file, which demonstrates how to run a model on the Cloud AI 100 platform.\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant Model as \"Trained Model\"\n    participant Input as \"Input Data\"\n    participant Cloud as \"Cloud AI 100 Platform\"\n    participant QPC as \"QPC File\"\n\n    Note over Model,Cloud: Compile and export model to ONNX\n    Model->>QPC: Compile ONNX to QPC\n    Input->>QPC: Process input data\n    QPC->>Cloud: Execute QPC on Cloud AI 100\n    Cloud->>QPC: Generate output\n```\n```mermaid\nclassDiagram\n    class Model {\n        -trained_model: TrainedModel\n    }\n    class Input {\n        -input_data: InputData\n    }\n    class Cloud {\n        -cloud_ai_100: CloudAI100\n    }\n    Model --* Input : uses\n    Input --* Cloud : runs on\n    Cloud --* Model : executes\n```\n## References\n* `QEfficient/cloud/infer.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `examples/granite_example/granite_vision_inference.py`"
  },
  "page-27": {
    "title": "Finetuning Models",
    "section": "Model Integration",
    "markdown": "# Finetuning Models\n## Overview\nFinetuning models is a crucial step in adapting pre-trained models to specific tasks or datasets. The Efficient Transformers library provides various tools and techniques to fine-tune models efficiently, as seen in the `QEfficient/transformers/models/modeling_auto.py` file.\n\n## Key Components / Concepts\nThe key components involved in finetuning models include the pre-trained model, the dataset, and the fine-tuning algorithm. The pre-trained model serves as the starting point, and the dataset is used to adapt the model to the specific task. The `examples/granite_example/granite_vision_inference.py` file demonstrates how to load and prepare a dataset for fine-tuning.\n\n## How it Works\nThe fine-tuning process involves adjusting the model's parameters to minimize the loss function on the target dataset. This can be done using various optimization algorithms and techniques, such as stochastic gradient descent (SGD) or Adam. The `tests/transformers/models/test_causal_lm_models.py` file contains examples of how to implement these optimization algorithms.\n\n## Example(s)\nAn example of fine-tuning a model can be seen in the `examples/granite_example/granite_vision_inference.py` file, where a pre-trained model is fine-tuned on a specific dataset. This example illustrates how to load a pre-trained model, prepare a dataset, and fine-tune the model using the Efficient Transformers library.\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant Pre-trained Model\n    participant Dataset\n    participant Fine-tuning Algorithm\n    Note over Pre-trained Model,Dataset: Load pre-trained model and dataset\n    Pre-trained Model->>Fine-tuning Algorithm: Initialize fine-tuning algorithm\n    Fine-tuning Algorithm->>Dataset: Train model on dataset\n    Note over Fine-tuning Algorithm,Dataset: Adjust model parameters to minimize loss\n    Fine-tuning Algorithm->>Pre-trained Model: Update model parameters\n```\nCaption: Fine-tuning process sequence diagram\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `examples/granite_example/granite_vision_inference.py`\n* `tests/transformers/models/test_causal_lm_models.py`\n* `QEfficient/transformers/optimization.py`"
  },
  "page-32": {
    "title": "Finetuning Models",
    "section": "Model Integration",
    "markdown": "# Finetuning Models\n## Overview\nFinetuning is a crucial step in model integration, allowing for the adaptation of pre-trained models to specific tasks or datasets. This process involves adjusting the model's weights to better fit the target task, resulting in improved performance.\n\n## Key Components / Concepts\nThe finetuning process typically involves the following key components:\n* Pre-trained model: The initial model that has been trained on a large dataset and is used as a starting point for finetuning.\n* Target task: The specific task or dataset that the model is being finetuned for.\n* Finetuning script: The code that implements the finetuning process, including the adjustment of model weights and the evaluation of model performance.\n\n## How it Works\nThe finetuning process works as follows:\n1. Load the pre-trained model and the target task dataset.\n2. Define the finetuning script, which includes the model architecture, loss function, and optimization algorithm.\n3. Train the model on the target task dataset, adjusting the model weights to minimize the loss function.\n4. Evaluate the model's performance on a validation set to monitor progress and prevent overfitting.\n\n## Example(s)\nFor example, to finetune a pre-trained language model on a specific dataset, you can use the following code:\n```python\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\n\n# Load pre-trained model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Define custom dataset class\nclass MyDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = tokenizer.encode_plus(\n            text,\n            max_length=512,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n# Create dataset and data loader\ndataset = MyDataset(texts, labels)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Finetune the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\nfor epoch in range(5):\n    model.train()\n    total_loss = 0\n    for batch in data_loader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}\")\n```\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant Pre-trained Model\n    participant Target Task Dataset\n    participant Finetuning Script\n    participant Model Weights\n\n    Pre-trained Model->>Finetuning Script: Load pre-trained model\n    Target Task Dataset->>Finetuning Script: Load target task dataset\n    Finetuning Script->>Model Weights: Adjust model weights\n    Model Weights->>Finetuning Script: Evaluate model performance\n    Finetuning Script->>Model Weights: Update model weights\n```\n## References\n* `scripts/finetune/__init__.py`\n* `scripts/finetune/run_ft_model.py`\n* `tests/transformers/models/test_causal_lm_models.py`"
  },
  "page-10": {
    "title": "Text Generation Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Text Generation Examples\n## Overview\nThis page provides examples of text generation using the models in the Efficient Transformers repository, located at `QEfficient/`.\n\n## Key Components / Concepts\nThe key components involved in text generation are the `TextGeneration` class and the `generate` function, defined in `QEfficient/generation/text_generation_inference.py`. The `TextGeneration` class initializes the model and tokenizer, while the `generate` function executes the model for a given list of prompts and a specified generation length.\n\n## How it Works\nThe text generation process involves the following steps:\n1. Initialize the `TextGeneration` class with the model and tokenizer.\n2. Prepare the input prompts and generation length.\n3. Call the `generate` function to execute the model.\n\n## Example(s)\nAn example of text generation can be seen in the `QEfficient/generation/text_generation_inference.py` file, where the `generate` function is used to generate text based on a given prompt. Additionally, `examples/intern_example/internvl_inference.py` demonstrates how to use the `TextGeneration` class for inference.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Initialize TextGeneration] --> B[Prepare Input Prompts]\n    B --> C[Call Generate Function]\n    C --> D[Execute Model]\n    D --> E[Generate Text]\n```\nCaption: Text Generation Flowchart\n\n## References\n* `QEfficient/generation/text_generation_inference.py`\n* `examples/intern_example/internvl_inference.py`\n* `QEfficient/utils/run_utils.py`\n* `tests/text_generation/test_text_generation.py`"
  },
  "page-16": {
    "title": "Finetuning Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Finetuning Examples\n## Overview\nFinetuning is a crucial step in adapting pre-trained models to specific tasks or datasets. QEfficient provides an efficient way to fine-tune models using the `QEfficient.cloud.finetune` module.\n\n## Key Components / Concepts\nThe key components involved in finetuning with QEfficient are:\n- `TrainConfig`: This object contains the configuration for training, including model name, learning rate, and dataset information.\n- `apply_peft`: This function applies Parameter-Efficient Fine-Tuning (PEFT) to the model if enabled.\n- `load_model_and_tokenizer`: This function loads the pre-trained model and tokenizer from Hugging Face.\n\n## How it Works\n1. The `main` function in `QEfficient/cloud/finetune.py` serves as the entry point for finetuning. It takes in various arguments, including model name, learning rate, and PEFT configuration.\n2. The `apply_peft` function is used to apply PEFT to the model if specified in the training configuration.\n3. The `load_model_and_tokenizer` function loads the pre-trained model and tokenizer based on the training configuration.\n\n## Example(s)\nTo fine-tune a model using QEfficient, you can use the following command:\n```bash\npython -m QEfficient.cloud.finetune \\\n    --model_name \"meta-llama/Llama-3.2-1B\" \\\n    --lr 5e-4 \\\n    --peft_config_file \"lora_config.yaml\"\n```\nThis command fine-tunes the Llama-3.2-1B model with a learning rate of 5e-4 and uses the PEFT configuration specified in `lora_config.yaml`.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Load Model and Tokenizer] -->|model, tokenizer|> B[Apply PEFT]\n    B -->|model|> C[Train Model]\n    C -->|trained model|> D[Save Model]\n```\nThis flowchart illustrates the process of loading a model and tokenizer, applying PEFT, training the model, and saving the trained model.\n\n## References\n- `QEfficient/cloud/finetune.py`\n- `QEfficient/transformers/models/modeling_auto.py`\n- `QEfficient/cloud/infer.py`"
  },
  "page-17": {
    "title": "Inference Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Inference Examples\n## Overview\nInference examples using QEfficient involve running different models on various inputs to generate outputs. This section provides an overview of the inference process and highlights key components and concepts.\n\n## Key Components / Concepts\nThe inference process in QEfficient involves several key components, including:\n* Models: QEfficient supports various models, such as language models and vision models.\n* Inputs: Inputs can be text prompts, images, or other types of data.\n* Outputs: Outputs are generated based on the input and model used.\n\n## How it Works\nThe inference process in QEfficient works as follows:\n1. Load the model and input data.\n2. Preprocess the input data if necessary.\n3. Run the model on the input data to generate output.\n4. Postprocess the output if necessary.\n\n## Example(s)\nExamples of inference using QEfficient can be found in the `examples` directory, including `draft_spd_inference.py`, `multiprojs_spd_inference.py`, and `pld_spd_inference.py`.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Load Input Data]\n    B --> C[Preprocess Input Data]\n    C --> D[Run Model]\n    D --> E[Postprocess Output]\n    E --> F[Generate Output]\n```\nInference Process Flowchart\n\n## References\n* `examples/draft_spd_inference.py`\n* `examples/multiprojs_spd_inference.py`\n* `QEfficient/cloud/infer.py`\n* `examples/pld_spd_inference.py`"
  },
  "page-18": {
    "title": "Model-Specific Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Model-Specific Examples\n## Overview\nThis section provides examples for specific models, such as LLaMA and Gemma, to demonstrate their usage and performance in the context of the Efficient Transformers Wiki repository.\n\n## Key Components / Concepts\nThe key components involved in these examples include the model architectures, input prompts, and output generation, which are crucial for understanding how the models function and produce results.\n\n## How it Works\nThe models are executed on the Cloud AI 100 platform, and the output is generated based on the input prompts, allowing for efficient processing and generation of text.\n\n## Example(s)\nFor example, the `llama4_example.py` script, located in the `examples` directory, demonstrates how to use the LLaMA model for text generation, showcasing its capabilities and potential applications.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Prompt] -->|Input Processing|> B[Model Execution]\n    B -->|Output Generation|> C[Output]\n    C -->|Post-processing|> D[Final Result]\n```\nThis flowchart illustrates the step-by-step process of generating output using a model, from input prompt to final result.\n\n## References\n* `examples/llama4_example.py`\n* `examples/gemma3_example/gemma3_mm.py`\n* `tests/transformers/models/test_causal_lm_models.py`\n* `README.md`"
  },
  "page-22": {
    "title": "QEfficientMPT Notebook",
    "section": "Examples and Notebooks",
    "markdown": "# QEfficientMPT Notebook\n## Overview\nThe QEfficientMPT notebook is designed to provide an overview of the QEfficientMPT model and its usage. This notebook is a key component of the Efficient Transformers Wiki and is intended to serve as a starting point for users looking to explore the capabilities of the QEfficientMPT model.\n\n## Key Components / Concepts\nThe QEfficientMPT model is a type of transformer model that is optimized for efficient processing. The key components of the QEfficientMPT model include the `CloudAI100ExecInfo` class, which holds information about Cloud AI 100 execution, and the `QAICInferenceSession` class, which is used to initialize a QAIC inference session.\n\n## How it Works\nThe QEfficientMPT model works by using a combination of techniques such as speculative decoding and multiprojection to improve efficiency. The model is designed to be highly customizable, allowing users to fine-tune its performance for specific use cases.\n\n## Example(s)\nTo use the QEfficientMPT model, users can follow these steps:\n1. Import the necessary libraries and load the model.\n2. Prepare the input data, including the prompts and any other relevant information.\n3. Use the `multiprojs_spec_decode_inference` function to perform speculative decode inference on the input data.\n4. Evaluate the results and fine-tune the model as needed.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Prepare Input Data]\n    B --> C[Perform Speculative Decode Inference]\n    C --> D[Evaluate Results]\n    D --> E[Fine-Tune Model]\n```\nThis flowchart illustrates the basic steps involved in using the QEfficientMPT model.\n\n## References\n* `QEfficient/generation/text_generation_inference.py`\n* `examples/multiprojs_spd_inference.py`\n* `notebooks/QEfficientMPT.ipynb`"
  },
  "page-23": {
    "title": "QEfficientGPT2 Notebook",
    "section": "Examples and Notebooks",
    "markdown": "# QEfficientGPT2 Notebook\n## Overview\nThe QEfficientGPT2 notebook is designed to provide an overview of the QEfficientGPT2 model and its usage. This notebook is part of the Efficient Transformers Wiki and is intended to serve as a guide for users who want to learn more about the QEfficientGPT2 model and how to use it.\n\n## Key Components / Concepts\nThe QEfficientGPT2 model is a type of transformer model that is designed to be efficient and scalable. It is based on the GPT2 architecture and has been optimized for use with the QEfficient framework. The model is capable of generating text and can be fine-tuned for specific tasks such as language translation and text summarization.\n\n## How it Works\nThe QEfficientGPT2 model works by using a combination of transformer layers and attention mechanisms to generate text. The model is trained on a large dataset of text and can be fine-tuned for specific tasks by adding additional layers and adjusting the hyperparameters.\n\n## Example(s)\nTo use the QEfficientGPT2 model, you can follow these steps:\n1. Import the necessary libraries and load the model.\n2. Prepare your input data and convert it to the correct format.\n3. Use the model to generate text based on your input data.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Input Data] -->|Preprocessing|> B[Model Input]\n    B -->|QEfficientGPT2 Model|> C[Generated Text]\n    C -->|Postprocessing|> D[Final Output]\n```\nThis diagram shows the basic workflow of the QEfficientGPT2 model, from input data to final output.\n\n## References\n* `notebooks/QEfficientGPT2.ipynb`\n* `QEfficient/generation/text_generation_inference.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `notebooks/__init__.py`"
  },
  "page-26": {
    "title": "Quick Start Guide",
    "section": "Examples and Notebooks",
    "markdown": "# Quick Start Guide\n## Overview\nThe Efficient Transformers library is designed to optimize transformer models for Cloud AI 100, providing a comprehensive solution for users. This guide provides a quick start for using the library, covering key components, concepts, and examples.\n\n## Key Components / Concepts\nThe library provides several key components, including the `QEFFAutoModel` class, which is used to manipulate transformer models from the HuggingFace hub. The `from_pretrained` method is used to initialize the model, and the `compile` method is used to compile the model for Cloud AI 100. These components work together to enable efficient transformer model usage.\n\n## How it Works\nTo use the library, first import the `QEFFAutoModel` class and initialize the model using the `from_pretrained` method. Then, compile the model using the `compile` method. The model can then be used for inference, allowing users to generate output based on input data.\n\n## Example(s)\n```python\nfrom QEfficient import QEFFAutoModel\nfrom transformers import AutoTokenizer\n\n# Initialize the model using from_pretrained\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Compile the model for Cloud AI 100\nmodel.compile(num_cores=16)\n\n# Prepare input\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\ninputs = tokenizer(\"My name is\", return_tensors=\"pt\")\n\n# Generate output\noutput = model.generate(inputs)\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Import QEFFAutoModel] --> B[Initialize Model]\n    B --> C[Compile Model]\n    C --> D[Prepare Input]\n    D --> E[Generate Output]\n```\nThe flowchart shows the steps involved in using the Efficient Transformers library, providing a visual representation of the process.\n\n## References\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[docs/source/quick_start.md](docs/source/quick_start.md)`"
  }
}