{
  "page-1": {
    "title": "Introduction to Efficient Transformers",
    "section": "Overview",
    "markdown": "# Introduction to Efficient Transformers\n## Overview\nThe Efficient Transformers library, located in the [QEfficient](QEfficient) directory, is a software library that provides support for various transformer models to improve efficiency in natural language processing tasks. It takes in input models and configurations from files such as [README.md](README.md) and [pyproject.toml](pyproject.toml), and outputs compiled models with optimized performance.\n\n## Key Components / Concepts\nThe library includes several key components, such as speculative decoding, quantization, and gradient checkpointing, which enable efficient inference of transformer models on Qualcomm Cloud AI 100. These components are implemented in files like [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py) and [QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py).\n\n## How it Works\nThe library works by transforming the input models into optimized models that can be run on the Cloud AI 100 hardware. This transformation involves replacing the original model layers with optimized implementations, as well as applying various techniques such as quantization and gradient checkpointing.\n\n## Example(s)\nFor example, the `QEFFAutoModel` class can be used to initialize a transformer model and compile it for Cloud AI 100. The `from_pretrained` method can be used to load a pre-trained model, and the `compile` method can be used to compile the model for Cloud AI 100.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] -->|Transform|> B[Optimized Model]\n    B -->|Compile|> C[Compiled Model]\n    C -->|Run|> D[Output]\n```\nThis flowchart shows the basic process of transforming an input model into an optimized model, compiling it for Cloud AI 100, and running it to produce output.\n\n## References\n* `[README.md](README.md)`\n* `[pyproject.toml](pyproject.toml)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`"
  },
  "page-4": {
    "title": "Introduction to Tests",
    "section": "Overview",
    "markdown": "# Introduction to Tests\n## Overview\nThe Efficient Transformers repository, located at `Efficient Transformers Wiki`, contains a comprehensive set of tests to ensure the correctness and consistency of its models and functionalities. These tests cover various aspects, including inference, export, compilation, and execution on different platforms, as outlined in `tests/README.md`.\n\n## Key Components / Concepts\nThe tests are designed to validate the performance of different models, such as PyTorch, ONNX, and Cloud AI 100, with and without continuous batching, as seen in `tests/transformers/models/test_causal_lm_models.py`. They also check the consistency of the models' output across different runs and platforms, ensuring reliability and accuracy.\n\n## How it Works\nThe tests are implemented using the Pytest framework and can be run using the `pytest` command. The tests are organized into different files and directories, each focusing on a specific aspect of the repository, such as `tests/transformers/spd/test_pld_inference.py` for testing inference.\n\n## Example(s)\nFor example, the `test_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100` function in `tests/transformers/models/test_causal_lm_models.py` validates the performance of different models, including PyTorch, PyTorch with KV changes, ONNX, and Cloud AI 100, with and without continuous batching.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Tests] --> B[Inference]\n    A --> C[Export]\n    A --> D[Compilation]\n    A --> E[Execution]\n    B --> F[PyTorch]\n    B --> G[ONNX]\n    B --> H[Cloud AI 100]\n    C --> I[Model Export]\n    D --> J[Model Compilation]\n    E --> K[Model Execution]\n```\nThis flowchart illustrates the different components of the tests and how they are related to each other, providing a visual representation of the testing process.\n\n## References\n* `tests/transformers/models/test_causal_lm_models.py`\n* `tests/README.md`\n* `tests/transformers/spd/test_pld_inference.py`\n* `docs/index.md`"
  },
  "page-11": {
    "title": "Introduction to QEfficient",
    "section": "Overview",
    "markdown": "# Introduction to QEfficient\n## Overview\nQEfficient is a library for efficient transformers, providing support for various models and features. It is designed to work with Hugging Face Transformer models for efficient inference on Qualcomm Cloud AI 100, utilizing file paths such as `QEfficient/transformers/models/modeling_auto.py` for model manipulation.\n\n## Key Components / Concepts\nThe library has several key components, including:\n* `QEFFAutoModel`: a class for manipulating any transformer model from the HuggingFace hub\n* `QEFFAutoModelForCausalLM`: a class for manipulating any causal language model from the HuggingFace hub\n* `transform_lm`: a function for replacing some Transformers torch.nn.Module layers with equivalent optimized modules for Cloud AI 100\n* `QEFFBaseModel`: a base class for all model classes, providing utility methods for child classes, as defined in `QEfficient/base/modeling_qeff.py`\n\n## How it Works\nThe library works by providing a framework for developers to work with transformer models. It takes in input models and configurations, and outputs compiled models with optimized performance. The library supports features such as speculative decoding, quantization, and gradient checkpointing.\n\n## Example(s)\nAn example of using the library is:\n```python\nfrom QEfficient import QEFFAutoModel\nfrom transformers import AutoTokenizer\n\n# Initialize the model using from_pretrained\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Compile the model for Cloud AI 100\nmodel.compile(num_cores=16)\n\n# Prepare input\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\ninputs = tokenizer(\"My name is\", return_tensors=\"pt\")\n\n# Execute the model\nmodel.generate(inputs)\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] --> B[QEfficient Library]\n    B --> C[Compiled Model]\n    C --> D[Cloud AI 100]\n    D --> E[Optimized Performance]\n```\nCaption: QEfficient Library Workflow\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/base/modeling_qeff.py`\n* `README.md`"
  },
  "page-20": {
    "title": "Introduction to Efficient Transformers",
    "section": "Overview",
    "markdown": "# Introduction to Efficient Transformers\n## Overview\nThe Efficient Transformers library, located in the `QEfficient` directory, is a software library that provides support for various transformer models to improve efficiency in natural language processing tasks. It takes in input models and configurations from files such as `README.md` and `QEfficient/transformers/transform.py`, and outputs compiled models with optimized performance.\n\n## Key Components / Concepts\nThe library includes several key components, such as speculative decoding, quantization, and gradient checkpointing, which enable efficient inference of transformer models on Qualcomm Cloud AI 100. These components are implemented in files like `QEfficient/transformers/models/modeling_auto.py`.\n\n## How it Works\nThe library works by providing a framework for developers to work with transformer models, including tools and configurations for testing, documentation, and quality assurance. The `pyproject.toml` file contains project metadata and build settings.\n\n## Example(s)\nFor example, the `QEFFAutoModel` class can be used to initialize a model using the `from_pretrained` method, and then compile the model for Cloud AI 100 using the `compile` method. This process is outlined in the `QEfficient/transformers/transform.py` file.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] -->|Transform|> B[Optimized Model]\n    B -->|Compile|> C[Compiled Model]\n    C -->|Deploy|> D[Cloud AI 100]\n```\nThis flowchart illustrates the process of transforming an input model into an optimized model, compiling it, and deploying it on Cloud AI 100.\n\n## References\n* `[README.md](README.md)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[pyproject.toml](pyproject.toml)`"
  },
  "page-23": {
    "title": "Introduction to Efficient Transformers",
    "section": "Overview",
    "markdown": "# Introduction to Efficient Transformers\n## Overview\nThe Efficient Transformers library, located in the `QEfficient/transformers` directory, is a software library that provides support for various transformer models to improve efficiency in natural language processing tasks. The library takes in input models and configurations from files such as `README.md` and outputs compiled models with optimized performance.\n\n## Key Components / Concepts\nThe library supports features such as speculative decoding, quantization, and gradient checkpointing, as implemented in `QEfficient/transformers/transform.py`. It also enables disaggregated serving, allowing for more efficient deployment of models in production environments, as described in `docs/source/introduction.md`.\n\n## How it Works\nThe library uses a combination of techniques to optimize the performance of transformer models, including replacing certain layers with optimized implementations in `QEfficient/transformers/models/modeling_auto.py`, applying quantization, and using speculative decoding to improve inference speed.\n\n## Example(s)\nAn example of using the Efficient Transformers library can be found in the `README.md` file, which provides a step-by-step guide on how to use the library to compile and deploy a transformer model.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Model] --> B[Optimization]\n    B --> C[Compiled Model]\n    C --> D[Deployment]\n```\nCaption: Overview of the Efficient Transformers library workflow.\n\n## References\n* `[README.md](README.md)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[docs/source/introduction.md](docs/source/introduction.md)`"
  },
  "page-29": {
    "title": "Introduction to Scripts",
    "section": "Overview",
    "markdown": "# Introduction to Scripts\n## Overview\nThe Efficient Transformers repository, located at `Efficient Transformers Wiki`, provides a variety of scripts to support different tasks and functionalities. These scripts are designed to be efficient, scalable, and easy to use, and can be found in the `scripts` directory.\n\n## Key Components / Concepts\nThe scripts in the repository can be broadly categorized into several key components, including:\n* Perplexity computation scripts, which calculate the perplexity of a given model using the `calculate_perplexity.py` script\n* Inference scripts, which perform inference tasks using the models, such as `draft_spd_inference.py` and `pld_spd_inference.py`\n* Finetune scripts, which fine-tune the models for specific tasks\n\n## How it Works\nThe scripts in the repository work by leveraging the Efficient Transformers library and its various components. For example, the perplexity computation scripts use the `calculate_perplexity.py` script to calculate the perplexity of a given model. The `draft_spd_inference.py` and `pld_spd_inference.py` scripts demonstrate how to perform inference tasks using the models.\n\n## Example(s)\nAn example of how to use the scripts can be seen in the `examples` directory, where scripts such as `draft_spd_inference.py` and `pld_spd_inference.py` demonstrate how to perform inference tasks using the models. The `calculate_perplexity.py` script can be used to calculate the perplexity of a given model.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[User] -->|runs script|> B[Script]\n    B -->|calls function|> C[Function]\n    C -->|performs task|> D[Task]\n    D -->|returns result|> B\n    B -->|returns result|> A\n```\nThis flowchart shows the basic flow of how a user interacts with the scripts in the repository.\n\n## References\n* `scripts/perplexity_computation/calculate_perplexity.py`\n* `examples/draft_spd_inference.py`\n* `examples/pld_spd_inference.py`\n* `docs/README.md`"
  },
  "page-2": {
    "title": "Docker Deployment",
    "section": "Deployment/Infrastructure",
    "markdown": "# Docker Deployment\n## Overview\nDocker deployment is a key aspect of the Efficient Transformers library, allowing for efficient and scalable deployment of transformer models. This guide provides an overview of the Docker deployment process for the Efficient Transformers library, which can be found in the `Efficient Transformers Wiki` repository under the `Deployment/Infrastructure` section.\n\n## Key Components / Concepts\nThe Efficient Transformers library uses Docker to create a containerized environment for deploying transformer models. The key components involved in the Docker deployment process include:\n* Dockerfile: The Dockerfile is used to create a Docker image that contains the Efficient Transformers library and its dependencies, as specified in the `pyproject.toml` file.\n* Docker image: The Docker image is created by building the Dockerfile and is used to deploy the Efficient Transformers library.\n* Container: The container is created by running the Docker image and provides a isolated environment for deploying the transformer models, such as those defined in `QEfficient/transformers/models/modeling_auto.py`.\n\n## How it Works\nThe Docker deployment process for the Efficient Transformers library works as follows:\n1. Create a Dockerfile that contains the instructions for building the Docker image.\n2. Build the Docker image using the Dockerfile.\n3. Create a container from the Docker image.\n4. Deploy the transformer models in the container, which can then be exported using scripts like `QEfficient/exporter/export_hf_to_cloud_ai_100.py`.\n\n## Example(s)\nAn example of a Dockerfile for the Efficient Transformers library is provided in the `Dockerfile` file, located in the root directory of the repository.\n\n## Diagram(s)\n```mermaid\ngraph TD\n    A[Dockerfile] -->|Build|> B[Docker Image]\n    B -->|Run|> C[Container]\n    C -->|Deploy|> D[Transformer Models]\n```\nCaption: Docker Deployment Process\n\n## References\n* `Dockerfile`\n* `pyproject.toml`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/exporter/export_hf_to_cloud_ai_100.py`"
  },
  "page-9": {
    "title": "Testing Cloud Deployment",
    "section": "Deployment/Infrastructure",
    "markdown": "# Testing Cloud Deployment\n## Overview\nTesting cloud deployment is a critical step in ensuring the efficient and reliable operation of cloud-based applications. This process involves verifying that the application can be successfully deployed to the cloud, and that it functions as expected in a cloud environment.\n\n## Key Components / Concepts\nThe key components of testing cloud deployment include:\n* Cloud infrastructure: The underlying infrastructure that supports the deployment of the application, such as virtual machines, containers, and serverless functions.\n* Deployment scripts: The scripts used to automate the deployment process, such as Terraform or CloudFormation.\n* Application code: The code that makes up the application, including any dependencies or libraries.\n\n## How it Works\nThe testing process typically involves the following steps:\n1. Preparation: The application code and deployment scripts are prepared for testing.\n2. Deployment: The application is deployed to the cloud using the deployment scripts.\n3. Verification: The application is verified to ensure that it is functioning as expected.\n4. Testing: The application is tested to ensure that it meets the required standards.\n\n## Example(s)\nFor example, the `cloud_ai_100_exec_kv` function in `QEfficient/generation/text_generation_inference.py` can be used to test the deployment of a cloud-based application.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Preparation] --> B[Deployment]\n    B --> C[Verification]\n    C --> D[Testing]\n    D --> E[Validation]\n```\nThis diagram shows the steps involved in testing cloud deployment.\n\n## References\n* `tests/transformers/spd/test_pld_inference.py`\n* `QEfficient/generation/text_generation_inference.py`\n* `QEfficient/cloud/execute.py`"
  },
  "page-15": {
    "title": "Cloud Deployment",
    "section": "Deployment/Infrastructure",
    "markdown": "# Cloud Deployment\n## Overview\nQEfficient provides tools for deploying models to the cloud, including export and inference scripts, allowing for efficient deployment of models on Cloud AI 100 hardware.\n\n## Key Components / Concepts\nThe key components involved in cloud deployment are:\n- Model export: The process of converting a model into a format compatible with Cloud AI 100, utilizing the `qualcomm_efficient_converter` function.\n- Model compilation: The process of compiling the exported model into a binary file that can be executed on Cloud AI 100 hardware, using the `compile` function.\n- Inference: The process of running the compiled model on Cloud AI 100 hardware to generate output, executed through the `execute_vlm_model` function.\n\n## How it Works\nThe cloud deployment process works as follows:\n1. The user exports their model using the `qualcomm_efficient_converter` function from `QEfficient/exporter/export_hf_to_cloud_ai_100.py`.\n2. The exported model is then compiled into a binary file using the `compile` function from `QEfficient/cloud/compile.py`.\n3. The compiled model is then executed on Cloud AI 100 hardware using the `execute_vlm_model` function from `QEfficient/cloud/infer.py`.\n\n## Example(s)\nAn example of how to deploy a model to the cloud can be found in the `docs/source/cli_api.md` file, which provides a step-by-step guide on utilizing the QEfficient tools for cloud deployment.\n\n## Diagram(s)\n```mermaid\ngraph TD\n    A[Model Export] -->|Export Model|> B[Model Compilation]\n    B -->|Compile Model|> C[Inference]\n    C -->|Generate Output|> D[Output]\n```\nCloud Deployment Process\n\n## References\n- `QEfficient/exporter/export_hf_to_cloud_ai_100.py`\n- `QEfficient/cloud/compile.py`\n- `QEfficient/cloud/infer.py`\n- `docs/source/cli_api.md`"
  },
  "page-28": {
    "title": "Requirements and Dependencies",
    "section": "Deployment/Infrastructure",
    "markdown": "# Requirements and Dependencies\n## Overview\nThe Efficient Transformers library has several requirements and dependencies for optimal performance. These include Python version 3.8-3.10, transformers, torch, and onnx, which are specified in the `pyproject.toml` and `docs/requirements.txt` files.\n\n## Key Components / Concepts\nThe library's key components include the QEFFAutoModel class, defined in `QEfficient/transformers/models/modeling_auto.py`, which is designed for manipulating transformer models from the Hugging Face hub, and the transform function, defined in `QEfficient/transformers/transform.py`, which optimizes models for Cloud AI 100.\n\n## How it Works\nThe library works by taking in input models and configurations, and outputting compiled models with optimized performance. It supports features such as speculative decoding, quantization, and gradient checkpointing, as described in the `docs/source/installation.md` file.\n\n## Example(s)\nAn example of using the library can be seen in the `QEfficient/transformers/models/modeling_auto.py` file, where the QEFFAutoModel class is defined. This class demonstrates how to manipulate transformer models and prepare them for optimization.\n\n## Diagram(s)\n```mermaid\ngraph TD\n    A[Hugging Face Model] -->|Input|> B[QEFFAutoModel]\n    B -->|Optimization|> C[Compiled Model]\n    C -->|Output|> D[Cloud AI 100]\n```\nThis diagram shows the flow of the library, from inputting a Hugging Face model to outputting a compiled model optimized for Cloud AI 100.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `pyproject.toml`\n* `docs/requirements.txt`"
  },
  "page-33": {
    "title": "Jenkinsfile",
    "section": "Deployment/Infrastructure",
    "markdown": "# Jenkinsfile\n## Overview\nThe Jenkinsfile is a crucial component in the deployment process of the Efficient Transformers project, located in the `Efficient Transformers Wiki` repository under `Deployment/Infrastructure`. It is a Groovy script that defines the pipeline for building, testing, and deploying the project.\n\n## Key Components / Concepts\nThe Jenkinsfile consists of several stages, including installation, testing, and deployment, which are defined in the `scripts/Jenkinsfile` file. Each stage has its own set of steps that are executed in sequence, as outlined in the `docs/source/installation.md` and `docs/source/quick_start.md` files.\n\n## How it Works\nThe Jenkinsfile works by defining a pipeline that is triggered by a specific event, such as a push to the `Efficient Transformers Wiki` repository. The pipeline then executes each stage in sequence, with each stage consisting of one or more steps, as configured in the `scripts/Jenkinsfile` file.\n\n## Example(s)\nAn example of a Jenkinsfile can be seen in the `scripts/Jenkinsfile` file, which defines a pipeline for building and testing the Efficient Transformers project. This file demonstrates how to structure the pipeline and define the various stages and steps.\n\n## Diagram(s)\n```mermaid\ngraph TD\n    A[Jenkinsfile] --> B[Installation]\n    B --> C[Testing]\n    C --> D[Deployment]\n    D --> E[Production]\n```\nThis diagram shows the basic flow of the Jenkinsfile, from installation to production, and illustrates how the different stages are connected.\n\n## References\n* `scripts/Jenkinsfile`\n* `docs/source/installation.md`\n* `docs/source/quick_start.md`\n* `Deployment/Infrastructure/Jenkinsfile`"
  },
  "page-3": {
    "title": "Library Configuration",
    "section": "Core Features",
    "markdown": "# Library Configuration\n## Overview\nThe Efficient Transformers library provides various configuration options to optimize model performance, leveraging key components such as `QEFFAutoModel`, `from_pretrained`, and `transform` to achieve optimal results. This section delves into the available configuration options, their usage, and the underlying mechanics.\n\n## Key Components / Concepts\nThe library's core functionality revolves around several pivotal components:\n* `QEFFAutoModel`: a class designed for manipulating transformer models sourced from the HuggingFace hub, facilitating seamless integration and customization.\n* `from_pretrained`: a method utilized for initializing models from pre-trained configurations, ensuring a solid foundation for further optimization.\n* `transform`: a function specifically engineered for optimizing models for deployment on Cloud AI 100, focusing on performance enhancement and efficiency.\n\n## How it Works\nThe configuration options play a crucial role in tailoring models to specific use cases, such as continuous batching or speculative decoding. The `from_pretrained` method serves as the initial step, loading pre-trained configurations into the model. Subsequently, the `transform` function is applied to fine-tune the model for Cloud AI 100, incorporating optimizations that cater to the cloud environment's unique requirements.\n\n## Example(s)\n### Initializing a Model\nAn example illustrating the use of the `from_pretrained` method to initialize a model:\n```python\nfrom QEfficient import QEFFAutoModel\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n```\n### Optimizing a Model for Cloud AI 100\nAn example demonstrating the use of the `transform` function to optimize a model for Cloud AI 100:\n```python\nfrom QEfficient.transformers import transform\nmodel = transform(model, form_factor=\"cloud\")\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Model Initialization] -->|from_pretrained|> B[Model Optimization]\n    B -->|transform|> C[Cloud AI 100 Optimization]\n    C -->|form_factor|> D[Optimized Model]\n```\nConfiguration Flowchart illustrating the sequence of model initialization, optimization, and transformation for Cloud AI 100.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/utils/_utils.py`\n* `QEfficient/README.md`"
  },
  "page-5": {
    "title": "Testing Causal LM Models",
    "section": "Core Features",
    "markdown": "# Testing Causal LM Models\n## Overview\nTesting causal language models (LMs) is crucial for ensuring their performance and consistency across different frameworks and configurations. This section describes the tests for causal LM models, focusing on their functionality and validation, as outlined in `tests/transformers/models/test_causal_lm_models.py`.\n\n## Key Components / Concepts\nThe key components involved in testing causal LM models include:\n- **Model Validation**: Verifying that the models produce the expected output for a given input, as seen in `tests/transformers/test_causal_lm.py`.\n- **Framework Comparison**: Comparing the performance of models across different frameworks, such as PyTorch and ONNX.\n- **Configuration Testing**: Testing models with various configurations, including different numbers of layers and attention implementations, defined in `tests/transformers/models/custom_tiny_model_configs.json`.\n\n## How it Works\nThe testing process involves the following steps:\n1. **Model Loading**: Loading the causal LM model from the Hugging Face repository, using the `test_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100` function.\n2. **Model Configuration**: Configuring the model with the desired settings, such as the number of hidden layers and attention implementation.\n3. **Input Generation**: Generating input data for the model, including input IDs, past key values, and other necessary inputs.\n4. **Model Evaluation**: Evaluating the model's performance using the generated input data.\n5. **Result Comparison**: Comparing the results from different models and frameworks to ensure consistency and accuracy.\n\n## Example(s)\nAn example of testing a causal LM model can be seen in the `test_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100` function, which validates the performance of different models, including PyTorch, ONNX, and Cloud AI 100, with and without continuous batching.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Configure Model]\n    B --> C[Generate Input Data]\n    C --> D[Evaluate Model]\n    D --> E[Compare Results]\n```\nThis flowchart illustrates the testing process for causal LM models, from loading the model to comparing the results.\n\n## References\n- `tests/transformers/models/test_causal_lm_models.py`\n- `tests/transformers/test_causal_lm.py`\n- `tests/transformers/models/custom_tiny_model_configs.json`\n- `tests/transformers/models/test_causal_lm_pytorch_vs_kv_vs_ort_vs_ai100.py`"
  },
  "page-6": {
    "title": "Testing Speech Seq2Seq Models",
    "section": "Core Features",
    "markdown": "# Testing Speech Seq2Seq Models\n## Overview\nTesting speech sequence-to-sequence (Seq2Seq) models is crucial for ensuring their performance and accuracy in various applications, including speech recognition and semantic search. This section provides an overview of the testing process for speech Seq2Seq models, which can be found in the `tests/transformers/models/test_speech_seq2seq_models.py` file.\n\n## Key Components / Concepts\nThe key components involved in testing speech Seq2Seq models include:\n- **Model Architecture**: The design of the Seq2Seq model, including the encoder, decoder, and any additional components such as attention mechanisms, as defined in `QEfficient/transformers/models/modeling_auto.py`.\n- **Test Data**: A dataset used to evaluate the model's performance, which should be representative of the tasks and scenarios the model is intended for.\n- **Evaluation Metrics**: Metrics used to measure the model's performance, such as accuracy, perplexity, or word error rate.\n\n## How it Works\nThe testing process for speech Seq2Seq models typically involves the following steps:\n1. **Model Initialization**: The Seq2Seq model is initialized with the desired architecture and parameters.\n2. **Data Preparation**: The test data is prepared and preprocessed as necessary, which may include tokenization, normalization, or other transformations.\n3. **Model Evaluation**: The model is evaluated on the test data using the chosen evaluation metrics.\n4. **Result Analysis**: The results of the evaluation are analyzed to determine the model's performance and identify areas for improvement.\n\n## Example(s)\nAn example of testing a speech Seq2Seq model can be seen in the `test_seq2seq_pretrained` function, which tests the functionality of a pre-trained sequence-to-sequence model for semantic search, as demonstrated in `tests/transformers/test_speech_seq2seq.py`.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Model Initialization] --> B[Data Preparation]\n    B --> C[Model Evaluation]\n    C --> D[Result Analysis]\n```\nThis flowchart illustrates the general process of testing a speech Seq2Seq model.\n\n## References\n- `tests/transformers/models/test_speech_seq2seq_models.py`\n- `tests/transformers/test_speech_seq2seq.py`\n- `QEfficient/transformers/models/modeling_auto.py`\n- `docs/core_features/testing_speech_seq2seq_models.md`"
  },
  "page-12": {
    "title": "Modeling Utilities",
    "section": "Core Features",
    "markdown": "# Modeling Utilities\n## Overview\nQEfficient provides various modeling utilities, including custom attention and rotary embedding implementations, located in `QEfficient/transformers/models/modeling_auto.py` and `QEfficient/base/modeling_qeff.py`. These utilities are designed to work with transformer models from the HuggingFace hub, such as those defined in `QEfficient/transformers/models/qwen3_moe/modeling_qwen3_moe.py`.\n\n## Key Components / Concepts\nThe key components of QEfficient's modeling utilities include:\n* Custom attention mechanisms defined in `QEfficient/transformers/transform.py`\n* Rotary embedding implementations\n* Support for various transformer models, including AutoModel and AutoModelForCausalLM\n\n## How it Works\nThe modeling utilities in QEfficient work by providing a set of classes and functions that can be used to manipulate and optimize transformer models. These utilities include methods for compiling and exporting models, as well as for generating text and images, as demonstrated in `tests/transformers/models/test_embedding_models.py`.\n\n## Example(s)\nAn example of how to use QEfficient's modeling utilities is as follows:\n```python\nfrom QEfficient import QEFFAutoModel\nfrom transformers import AutoTokenizer\n\n# Initialize the model using from_pretrained\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Prepare input\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer(\"My name is\", return_tensors=\"pt\")\n\n# Generate text\noutput = model.generate(inputs)\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Model Initialization] --> B[Model Compilation]\n    B --> C[Model Export]\n    C --> D[Text Generation]\n    D --> E[Output]\n```\nThis diagram shows the basic workflow of QEfficient's modeling utilities, from model initialization to text generation.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/base/modeling_qeff.py`\n* `QEfficient/transformers/models/qwen3_moe/modeling_qwen3_moe.py`"
  },
  "page-13": {
    "title": "Quantization and Compression",
    "section": "Core Features",
    "markdown": "# Quantization and Compression\n## Overview\nQEfficient supports various quantization and compression techniques, including FP8 and AWQ, to optimize model performance. These techniques enable the reduction of model size and improvement of inference speed.\n\n## Key Components / Concepts\nThe key components involved in quantization and compression are:\n- `QEffExtendedQuantizationMethod`: An enumeration of quantization methods, including FP8, which defines the quantization approach used for model optimization.\n- `QEffCompressedTensorsFP8Quantizer`: A class for quantizing compressed tensors using FP8, providing a specific implementation for quantization.\n\n## How it Works\nQuantization and compression in QEfficient work by replacing the torch.nn.Module layers of a model with optimized implementations. This is achieved through the `transform_lm` function, which replaces some Transformers torch.nn.Module layers with equivalent optimized modules for Cloud AI 100. The process involves analyzing the model architecture, identifying layers that can be optimized, and applying the appropriate quantization and compression techniques.\n\n## Example(s)\nAn example of using quantization and compression can be seen in the `QEffCompressedTensorsFP8Quantizer` class, which quantizes compressed tensors using FP8. This class demonstrates how to apply quantization to compressed tensors, resulting in a reduced model size and improved inference speed.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Model] -->|transform_lm|> B[Optimized Model]\n    B -->|QEffCompressedTensorsFP8Quantizer|> C[Quantized Model]\n    C -->|Compression|> D[Compressed Model]\n```\nQuantization and Compression Flowchart\n\n## References\n- `QEfficient/transformers/quantizers/quantizer_compressed_tensors.py`\n- `QEfficient/transformers/transform.py`\n- `QEfficient/transformers/models/modeling_auto.py`"
  },
  "page-19": {
    "title": "Model Integration",
    "section": "Core Features",
    "markdown": "# Model Integration\n## Overview\nThe Efficient Transformers library supports the integration of various models, including LLaMA, Gemma, and Granite, by utilizing the HuggingFace hub. This section provides an overview of the model integration process, which enables users to optimize and compile models for Cloud AI 100.\n\n## Key Components / Concepts\nThe key components involved in model integration are:\n* `QEFFAutoModel`: a class designed for manipulating any transformer model from the HuggingFace hub, located in `QEfficient/transformers/models/modeling_auto.py`.\n* `from_pretrained`: a method that serves as the easiest entry point into using QEfficient, defined in `QEfficient/transformers/models/modeling_auto.py`.\n* `transform`: a function that optimizes any kind of model for Cloud AI 100, implemented in `QEfficient/transformers/transform.py`.\n\n## How it Works\nThe model integration process involves the following steps:\n1. Initialize the model using `from_pretrained`, which loads the pre-trained model from the HuggingFace hub.\n2. Optimize the model for Cloud AI 100 using `transform`, which applies optimizations to the model architecture.\n3. Compile the model for Cloud AI 100, which prepares the model for execution on the target hardware.\n\n## Example(s)\nAn example of model integration can be found in the `examples/granite_example/granite_vision_inference.py` file, which demonstrates how to integrate the Granite model with the Efficient Transformers library.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Initialize Model] --> B[Optimize Model]\n    B --> C[Compile Model]\n    C --> D[Execute Model]\n```\nModel Integration Flowchart\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `examples/granite_example/granite_vision_inference.py`"
  },
  "page-24": {
    "title": "Python API Reference",
    "section": "Core Features",
    "markdown": "# Python API Reference\n## Overview\nThe Efficient Transformers library provides a Python API for manipulating and optimizing transformer models, located in `QEfficient/transformers/models/modeling_auto.py`. The API is designed to be similar to the Hugging Face Transformers library, with additional features and optimizations for Cloud AI 100.\n\n## Key Components / Concepts\nThe key components of the Python API are:\n* `QEFFAutoModel`: a class for manipulating any transformer model from the Hugging Face hub, defined in `QEfficient/transformers/models/modeling_auto.py`.\n* `QEFFAutoModelForCausalLM`: a class for manipulating causal language models from the Hugging Face hub, also defined in `QEfficient/transformers/models/modeling_auto.py`.\n* `transform`: a function for optimizing any kind of model for Cloud AI 100, implemented in `QEfficient/transformers/transform.py`.\n\n## How it Works\nThe API works by providing a set of classes and functions that can be used to manipulate and optimize transformer models. The `QEFFAutoModel` and `QEFFAutoModelForCausalLM` classes provide methods for initializing, compiling, and generating text using transformer models. The `transform` function optimizes the model for Cloud AI 100 by replacing the torch.nn.Module layers with optimized implementation of the same, as described in `QEfficient/base/modeling_qeff.py`.\n\n## Example(s)\n```python\nfrom QEfficient import QEFFAutoModel\nfrom transformers import AutoTokenizer\n\n# Initialize the model using from_pretrained\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Compile the model for Cloud AI 100\nmodel.compile(num_cores=16)\n\n# Prepare input\ntokenizer = AutoTokenizer.from_pretrained(\"model_name\")\ninputs = tokenizer(\"My name is\", return_tensors=\"pt\")\n\n# Generate text\nmodel.generate(inputs)\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Compile Model]\n    B --> C[Prepare Input]\n    C --> D[Generate Text]\n    D --> E[Optimize Model]\n    E --> F[Deploy Model]\n```\nOptimization Flowchart\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/transform.py`\n* `QEfficient/base/modeling_qeff.py`\n* `docs/source/python_api.md`"
  },
  "page-25": {
    "title": "CLI API Reference",
    "section": "Core Features",
    "markdown": "# CLI API Reference\n## Overview\nThe CLI API of the Efficient Transformers library provides a simple and efficient way to interact with the library's functionality, allowing users to optimize models, export and compile models, and perform other tasks.\n\n## Key Components / Concepts\nThe CLI API consists of several key components, including:\n* `transform.py`: This module contains functions for optimizing models, such as `transform` and `transform_lm`.\n* `modeling_auto.py`: This module contains classes for working with models, such as `QEFFAutoModel` and `QEFFAutoModelForCausalLM`.\n* `utils/_utils.py`: This module contains utility functions, such as `load_yaml` and `load_json`.\n\n## How it Works\nThe CLI API works by providing a set of commands that can be used to interact with the library's functionality, including optimizing models, exporting and compiling models, and performing other tasks.\n\n## Example(s)\nHere is an example of how to use the CLI API to optimize a model:\n```python\nfrom QEfficient.transformers.transform import transform\n\n# Load the model\nmodel = ...\n\n# Optimize the model\noptimized_model = transform(model, form_factor=\"cloud\")\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Load Model] --> B[Optimize Model]\n    B --> C[Export Model]\n    C --> D[Compile Model]\n    D --> E[Run Model]\n```\nThis diagram shows the basic workflow of the CLI API, from loading a model to running it.\n\n## References\n* `QEfficient/transformers/transform.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/utils/_utils.py`\n* `docs/source/cli_api.md`"
  },
  "page-30": {
    "title": "Perplexity Computation",
    "section": "Core Features",
    "markdown": "# Perplexity Computation\n## Overview\nPerplexity computation is a crucial aspect of evaluating the performance of language models. It measures how well a model can predict the next word in a sequence, given the context of the previous words.\n\n## Key Components / Concepts\nThe perplexity computation script utilizes several key components, including the `calculate_perplexity` function, which takes in parameters such as data loader, inference session, context length, prompt length, model type, batch size, log file, and model name. These parameters are defined in the `scripts/perplexity_computation/calculate_perplexity.py` file.\n\n## How it Works\nThe script works by iterating over the data loader, calculating the loss for each batch using the `torch_perplexity` function for PyTorch models, and then computing the perplexity using the average loss. The `tests/transformers/test_causal_lm.py` file provides an example of how to use the perplexity computation script for testing purposes.\n\n## Example(s)\nTo use the perplexity computation script, simply call the `calculate_perplexity` function with the required parameters. For example:\n```python\ncalculate_perplexity(data_loader, inference_session, ctx_len, prompt_len, model_type, batch_size, log_file, model_name)\n```\nThis function is imported from the `scripts/perplexity_computation/__init__.py` file, which serves as the main entry point for the perplexity computation module.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Data Loader] -->|input_ids, attention_mask|> B[Inference Session]\n    B -->|outputs|> C[Loss Calculation]\n    C -->|loss|> D[Perplexity Calculation]\n    D -->|perplexity|> E[Log File]\n```\nPerplexity computation flowchart.\n\n## References\n* `scripts/perplexity_computation/calculate_perplexity.py`\n* `scripts/perplexity_computation/__init__.py`\n* `tests/transformers/test_causal_lm.py`\n* `QEfficient/utils/_utils.py`"
  },
  "page-31": {
    "title": "Replicating KV Heads",
    "section": "Core Features",
    "markdown": "# Replicating KV Heads\n## Overview\nReplicating KV heads is a process that involves duplicating the key-value heads in a transformer model to improve its performance. This process is particularly useful for auto-regressive tasks where sequence generation involves processing one token at a time.\n\n## Key Components / Concepts\nThe key components involved in replicating KV heads include:\n* The `replicate_kv_heads` function, which takes in a model name, prompt, and repeat factor as inputs and returns the modified model with replicated KV heads.\n* The `duplicate_weights_for_linear_layer` function, which duplicates the weights for a linear layer in the model.\n\n## How it Works\nThe process of replicating KV heads works as follows:\n1. The `replicate_kv_heads` function is called with the model name, prompt, and repeat factor as inputs.\n2. The function runs inference with the original model.\n3. The function replicates the KV heads by duplicating the weights for the linear layers in the model.\n4. The function runs inference on the modified model to validate the changes.\n5. The function exports the modified model to ONNX format.\n\n## Example(s)\nAn example of replicating KV heads can be seen in the `scripts/replicate_kv_head/replicate_kv_heads.py` file, where the `replicate_kv_heads` function is defined.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Run Inference with Original Model] --> B[Replicate KV Heads]\n    B --> C[Run Inference with Modified Model]\n    C --> D[Export Modified Model to ONNX]\n```\nThis flowchart illustrates the process of replicating KV heads, from running inference with the original model to exporting the modified model to ONNX format.\n\n## References\n* `scripts/replicate_kv_head/replicate_kv_heads.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `tests/transformers/models/test_causal_lm_models.py`\n* `docs/core_features/replicating_kv_heads.md`"
  },
  "page-7": {
    "title": "Testing PEFT Models",
    "section": "Model Integration",
    "markdown": "# Testing PEFT Models\n## Overview\nTesting PEFT (Parameter-Efficient Fine-Tuning) models is crucial to ensure their performance and accuracy in various tasks. PEFT models are designed to be efficient and adaptable, making them suitable for a wide range of applications.\n\n## Key Components / Concepts\nThe key components involved in testing PEFT models include the model architecture, the adapter, and the testing framework. The model architecture refers to the underlying structure of the PEFT model, while the adapter is responsible for fine-tuning the model for specific tasks. The testing framework provides a set of tools and methodologies for evaluating the performance of the PEFT model.\n\n## How it Works\nThe testing process for PEFT models typically involves the following steps:\n1. **Model Initialization**: The PEFT model is initialized with a pre-trained base model and an adapter.\n2. **Adapter Fine-Tuning**: The adapter is fine-tuned for a specific task using a dataset.\n3. **Model Evaluation**: The performance of the PEFT model is evaluated using a testing framework.\n\n## Example(s)\nAn example of testing a PEFT model can be seen in the `create_peft_model` function, which creates a PEFT model for semantic search by combining a base model with an adapter.\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant PEFTModel as \"PEFT Model\"\n    participant BaseModel as \"Base Model\"\n    participant Adapter as \"Adapter\"\n    participant TestingFramework as \"Testing Framework\"\n    Note over PEFTModel,BaseModel: Model Initialization\n    PEFTModel->>BaseModel: Load pre-trained base model\n    PEFTModel->>Adapter: Initialize adapter\n    Note over Adapter,TestingFramework: Adapter Fine-Tuning\n    Adapter->>TestingFramework: Fine-tune adapter using dataset\n    Note over PEFTModel,TestingFramework: Model Evaluation\n    PEFTModel->>TestingFramework: Evaluate PEFT model performance\n```\n```mermaid\nclassDiagram\n    class PEFTModel {\n        -base_model: BaseModel\n        -adapter: Adapter\n        +fine_tune(): void\n        +evaluate(): void\n    }\n    class BaseModel {\n        +get_weights(): Weights\n    }\n    class Adapter {\n        +get_weights(): Weights\n    }\n    PEFTModel --* BaseModel\n    PEFTModel --* Adapter\n```\nThis sequence diagram illustrates the steps involved in testing a PEFT model, and the class diagram shows the relationship between the PEFT model, the base model, and the adapter.\n\n## References\n* `tests/peft/test_peft_model.py`\n* `tests/peft/test_peft_onnx_transforms.py`\n* `QEfficient/finetune/utils/helper.py`\n* `models/peft/peft_model.py`"
  },
  "page-8": {
    "title": "Testing Embedding Models",
    "section": "Model Integration",
    "markdown": "# Testing Embedding Models\n## Overview\nTesting embedding models is a crucial step in ensuring the accuracy and reliability of semantic search applications. This process involves validating the output of different model runtimes, such as PyTorch, ONNX, and AI 100, to guarantee consistency and accuracy. The testing process is grounded in the file paths provided in the Efficient Transformers Wiki repository.\n\n## Key Components / Concepts\nThe key components involved in testing embedding models include:\n- **Model Name**: The name of the Hugging Face model being tested, as specified in `tests/transformers/models/test_embedding_models.py`.\n- **Sequence Length**: The length of the input sequence, which affects the model's performance.\n- **Number of Layers**: The number of layers in the model, which impacts the complexity of the model.\n- **Pooling Method**: The method used for pooling, such as mean or max pooling, as implemented in `QEfficient/transformers/models/modeling_auto.py`.\n\n## How it Works\nThe testing process typically involves the following steps:\n1. **Model Initialization**: Initialize the model using the `from_pretrained` method, as shown in `tests/transformers/models/test_embedding_models.py`.\n2. **Input Preparation**: Prepare the input data, including tokenization and conversion to the required format.\n3. **Model Output**: Generate the output from the model using the `generate` method.\n4. **Output Processing**: Process the output, including pooling and calculation of the mean absolute difference (MAD) between the embeddings generated by each model.\n\n## Example(s)\nAn example of testing an embedding model can be seen in the `test_embed_model_pytorch_vs_onnx_vs_ai100` function, which validates the output of PyTorch, ONNX, and AI 100 runtime models for semantic search. This function is implemented in `tests/transformers/models/test_embedding_models.py`.\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant Model as \"Hugging Face Model\"\n    participant Tester as \"Tester\"\n    participant ONNX as \"ONNX Runtime\"\n    participant AI100 as \"AI 100 Runtime\"\n\n    Tester->>Model: Initialize model\n    Model->>Tester: Return model object\n    Tester->>Model: Prepare input data\n    Model->>Tester: Return output\n    Tester->>ONNX: Export model to ONNX\n    ONNX->>Tester: Return ONNX model\n    Tester->>AI100: Compile ONNX model\n    AI100->>Tester: Return compiled model\n    Tester->>AI100: Run inference on compiled model\n    AI100->>Tester: Return output\n    Tester->>Tester: Calculate MAD between outputs\n```\nCaption: Sequence diagram showing the testing process for embedding models.\n\n## References\n- `tests/transformers/models/test_embedding_models.py`\n- `QEfficient/transformers/models/modeling_auto.py`\n- `tests/peft/test_peft_onnx_transforms.py`\n- `tests/transformers/models/test_causal_lm_models.py`"
  },
  "page-14": {
    "title": "Model Implementations",
    "section": "Model Integration",
    "markdown": "# Model Implementations\n## Overview\nQEfficient provides implementations for various models, including LLaMA, Gemma, and Whisper, integrated into the QEfficient framework for efficient and scalable processing. The models are loaded from Hugging Face using the `from_pretrained` method.\n\n## Key Components / Concepts\nThe key components of QEfficient model implementations include:\n* `QEFFAutoModelForCausalLM`: a class for loading models with multiple LoRA adapters, located in [QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)\n* `QEFFAutoModelForImageTextToText`: a class for working with multimodal language models, utilizing the `kv_offload` method for optimization\n* `from_pretrained`: a method for loading pre-trained models from Hugging Face, as seen in [QEfficient/peft/lora/auto.py](QEfficient/peft/lora/auto.py)\n\n## How it Works\nThe QEfficient model implementations work by loading pre-trained models from Hugging Face and applying various transformations and optimizations, such as LoRA adapters and `kv_offload`, to enable efficient processing on Cloud AI 100 hardware. This process is facilitated by the `QEFFAutoModelForCausalLM` and `QEFFAutoModelForImageTextToText` classes.\n\n## Example(s)\nAn example of using QEfficient to load a pre-trained LLaMA model is:\n```python\nfrom QEfficient import QEFFAutoModelForCausalLM\n\nmodel = QEFFAutoModelForCausalLM.from_pretrained(\"llama-hf/llama-1.5-7b-hf\")\n```\nAdditionally, the `QEFFAutoModelForImageTextToText` class can be used to work with multimodal language models, as demonstrated in [QEfficient/transformers/models/gemma/modeling_gemma.py](QEfficient/transformers/models/gemma/modeling_gemma.py).\n\n## Diagram(s)\n```mermaid\nclassDiagram\n    class QEFFAutoModelForCausalLM {\n        +from_pretrained()\n        +load_adapter()\n        +compile()\n    }\n    class QEFFAutoModelForImageTextToText {\n        +from_pretrained()\n        +kv_offload()\n    }\n    QEFFAutoModelForCausalLM --|> QEFFAutoModelForImageTextToText\n```\nCaption: QEfficient Model Implementation Class Diagram\n\n## References\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[QEfficient/peft/lora/auto.py](QEfficient/peft/lora/auto.py)`\n* `[QEfficient/transformers/models/gemma/modeling_gemma.py](QEfficient/transformers/models/gemma/modeling_gemma.py)`\n* `[QEfficient/transformers/models/whisper/modeling_whisper.py](QEfficient/transformers/models/whisper/modeling_whisper.py)`"
  },
  "page-27": {
    "title": "Model Fine-Tuning",
    "section": "Model Integration",
    "markdown": "# Model Fine-Tuning\n## Overview\nModel fine-tuning is a crucial step in the Efficient Transformers library, allowing users to adapt pre-trained models to their specific tasks. This process involves adjusting the model's weights to fit the target dataset, resulting in improved performance and accuracy.\n\n## Key Components / Concepts\nThe key components involved in model fine-tuning are:\n* Pre-trained models: These are models that have been trained on large datasets and can be used as a starting point for fine-tuning.\n* Target dataset: This is the dataset that the user wants to fine-tune the model for.\n* Hyperparameters: These are parameters that control the fine-tuning process, such as learning rate and batch size.\n\n## How it Works\nThe fine-tuning process typically involves the following steps:\n1. Load the pre-trained model and target dataset.\n2. Prepare the dataset for fine-tuning by preprocessing and formatting the data.\n3. Define the hyperparameters for the fine-tuning process.\n4. Train the model on the target dataset using the defined hyperparameters.\n5. Evaluate the model's performance on a validation set to monitor progress and adjust hyperparameters as needed.\n\n## Example(s)\nAn example of fine-tuning a model using the Efficient Transformers library can be seen in the `tests/transformers/models/test_causal_lm_models.py` file, which contains functions for testing and fine-tuning causal language models.\n\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant Pre-trained Model\n    participant Target Dataset\n    participant Hyperparameters\n    participant Fine-tuning Process\n    participant Evaluation\n\n    Note over Pre-trained Model,Target Dataset: Load pre-trained model and target dataset\n    Pre-trained Model->>Fine-tuning Process: Load model\n    Target Dataset->>Fine-tuning Process: Load dataset\n    Hyperparameters->>Fine-tuning Process: Define hyperparameters\n    Fine-tuning Process->>Evaluation: Train and evaluate model\n    Evaluation->>Fine-tuning Process: Monitor progress and adjust hyperparameters\n```\nThis sequence diagram illustrates the fine-tuning process, from loading the pre-trained model and target dataset to training and evaluating the model.\n\n## References\n* `tests/transformers/models/test_causal_lm_models.py`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `examples/granite_example/granite_vision_inference.py`\n* `docs/model_integration/model_fine_tuning.md`"
  },
  "page-32": {
    "title": "Finetuning Models",
    "section": "Model Integration",
    "markdown": "# Finetuning Models\n## Overview\nFinetuning models is a crucial step in adapting pre-trained models to specific tasks or datasets. The `QEfficient` library provides a finetuning script that allows users to fine-tune models on QAIC hardware with configurable training and LoRA parameters.\n\n## Key Components / Concepts\nThe finetuning process involves several key components, including:\n* `TrainConfig`: a configuration object that contains model and tokenizer names, as well as other training parameters.\n* `load_model_and_tokenizer`: a function that loads the pre-trained model and tokenizer from Hugging Face.\n* `apply_peft`: a function that applies Parameter-Efficient Fine-Tuning (PEFT) to the model if enabled.\n\n## How it Works\nThe finetuning script works by first loading the pre-trained model and tokenizer using `load_model_and_tokenizer`. It then applies PEFT to the model using `apply_peft` if enabled. The script then sets up the training configuration using `TrainConfig` and updates the configuration with any additional arguments provided by the user.\n\n## Example(s)\nAn example of how to use the finetuning script can be found in the `QEfficient/cloud/finetune.py` file. The script can be run using the following command:\n```bash\npython -m QEfficient.cloud.finetune \\\n    --model_name \"meta-llama/Llama-3.2-1B\" \\\n    --lr 5e-4 \\\n    --peft_config_file \"lora_config.yaml\"\n```\n## Diagram(s)\n```mermaid\nsequenceDiagram\n    participant User as \"User\"\n    participant FinetuneScript as \"Finetune Script\"\n    participant HuggingFace as \"Hugging Face\"\n    participant QAIC as \"QAIC Hardware\"\n\n    User->>FinetuneScript: Run finetuning script\n    FinetuneScript->>HuggingFace: Load pre-trained model and tokenizer\n    HuggingFace->>FinetuneScript: Return model and tokenizer\n    FinetuneScript->>FinetuneScript: Apply PEFT (if enabled)\n    FinetuneScript->>QAIC: Set up training configuration\n    QAIC->>FinetuneScript: Train model\n    FinetuneScript->>User: Return trained model\n```\n## References\n* `QEfficient/cloud/finetune.py`\n* `tests/transformers/models/test_causal_lm_models.py`\n* `QEfficient/cloud/train_config.py`"
  },
  "page-10": {
    "title": "Text Generation Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Text Generation Examples\n## Overview\nThe Efficient Transformers repository provides various examples of text generation using different models and techniques. This page showcases some of these examples, including code snippets and explanations, which can be found in files such as `QEfficient/generation/text_generation_inference.py` and `examples/intern_example/internvl_inference.py`.\n\n## Key Components / Concepts\nThe key components involved in text generation are:\n* **Models**: The repository includes various models, such as GPT-2 and TinyLlama, which can be used for text generation.\n* **Tokenizers**: Tokenizers are used to preprocess the input text and convert it into a format that can be understood by the model.\n* **Inference**: Inference refers to the process of generating text using a trained model.\n\n## How it Works\nThe text generation process typically involves the following steps:\n1. **Preprocessing**: The input text is preprocessed using a tokenizer to convert it into a format that can be understood by the model.\n2. **Model Execution**: The preprocessed input is passed through the model to generate output.\n3. **Postprocessing**: The generated output is postprocessed to convert it back into a human-readable format.\n\n## Example(s)\nHere is an example of text generation using the `generate` function from the `QEfficient/generation/text_generation_inference.py` file:\n```python\nfrom QEfficient.generation.text_generation_inference import TextGeneration\n\n# Initialize the TextGeneration class\ntext_generation = TextGeneration(tokenizer, qpc_path, full_batch_size, ctx_len, device_id, enable_debug_logs, write_io_dir, is_tlm, include_sampler, return_pdfs, sampling_params)\n\n# Generate text using the model\ngenerated_text = text_generation.generate(prompt, generation_len, stream, prompt_to_lora_id_mapping)\n```\nThis example demonstrates how to use the `TextGeneration` class to generate text.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Input Text] -->|Preprocessing|> B[Tokenized Input]\n    B -->|Model Execution|> C[Generated Output]\n    C -->|Postprocessing|> D[Final Output]\n```\nThis flowchart illustrates the text generation process, from input text to final output, and is based on the implementation in `QEfficient/generation/text_generation_inference.py`.\n\n## References\n* `QEfficient/generation/text_generation_inference.py`\n* `examples/intern_example/internvl_inference.py`\n* `tests/text_generation/test_text_generation.py`\n* `QEfficient/models/gpt2.py`"
  },
  "page-16": {
    "title": "Finetuning Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Finetuning Examples\n## Overview\nQEfficient provides examples for finetuning models, including scripts and configuration files. These examples demonstrate how to fine-tune pre-trained models on specific tasks and datasets.\n\n## Key Components / Concepts\nThe key components involved in finetuning examples include:\n* Pre-trained models: These are models that have been trained on large datasets and can be fine-tuned for specific tasks.\n* Configuration files: These files contain hyperparameters and other settings for fine-tuning the models.\n* Scripts: These are Python scripts that run the fine-tuning process using the pre-trained models and configuration files.\n\n## How it Works\nThe finetuning process involves the following steps:\n1. Load the pre-trained model and configuration file.\n2. Prepare the dataset for fine-tuning.\n3. Fine-tune the model on the dataset using the configuration settings.\n4. Evaluate the fine-tuned model on a validation set.\n\n## Example(s)\nAn example of finetuning a model can be found in the `QEfficient/cloud/finetune.py` file. This script fine-tunes a pre-trained model on a dataset using a configuration file.\n\n## Diagram(s)\n```mermaid\nflowchart LR\n    A[Load Pre-trained Model] --> B[Prepare Dataset]\n    B --> C[Fine-tune Model]\n    C --> D[Evaluate Model]\n```\nThis flowchart shows the steps involved in finetuning a model.\n\n## References\n* `QEfficient/cloud/finetune.py`\n* `QEfficient/finetune/configs/peft_config.py`\n* `QEfficient/finetune/configs/training.py`\n* `QEfficient/docs/README.md`"
  },
  "page-17": {
    "title": "Inference Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Inference Examples\n## Overview\nInference examples using various models, including LLaMA, Gemma, and Granite, are provided to demonstrate the usage of these models for different tasks.\n\n## Key Components / Concepts\nThe key components involved in these examples are the models themselves, the input prompts, and the device groups used for inference.\n\n## How it Works\nThe inference process involves compiling the target model, initializing a QAIC inference session, and running prefill on the model to generate output logits.\n\n## Example(s)\nExamples of inference using LLaMA, Gemma, and Granite models can be found in the following files:\n- `examples/llama4_example.py`\n- `examples/gemma3_example/gemma3_mm.py`\n- `examples/granite_example/granite_vision_inference.py`\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Input Prompts] -->|Compiled Model|> B[QAIC Inference Session]\n    B -->|Prefill|> C[Output Logits]\n    C -->|Generated Text|> D[Final Output]\n```\nInference Flowchart\n\n## References\n- `examples/llama4_example.py`\n- `examples/gemma3_example/gemma3_mm.py`\n- `examples/granite_example/granite_vision_inference.py`\n- `README.md`"
  },
  "page-18": {
    "title": "Speech-to-Text Examples",
    "section": "Examples and Notebooks",
    "markdown": "# Speech-to-Text Examples\n## Overview\nThis page provides examples of speech-to-text inference using Whisper and other models, highlighting their applications and usage within the Efficient Transformers Wiki repository.\n\n## Key Components / Concepts\nThe key components involved in speech-to-text inference are:\n- **Audio Input**: The audio file or stream that needs to be transcribed, which can be found in files such as `examples/speech_to_text/audio_samples.wav`.\n- **Model**: The speech-to-text model used for transcription, such as Whisper, which is utilized in `examples/speech_to_text/run_whisper_speech_to_text.py`.\n- **Processor**: The processor used to preprocess the audio input and postprocess the model output, as demonstrated in `tests/transformers/test_speech_seq2seq.py`.\n\n## How it Works\nThe speech-to-text inference process works as follows:\n1. **Audio Input**: The audio file or stream is input into the system, as shown in `examples/speech_to_text/run_whisper_speech_to_text.py`.\n2. **Preprocessing**: The audio input is preprocessed using a processor to extract features that can be used by the model.\n3. **Model Inference**: The preprocessed audio features are input into the model, which generates a transcript.\n4. **Postprocessing**: The model output is postprocessed using a processor to generate the final transcript.\n\n## Example(s)\nAn example of speech-to-text inference using Whisper can be found in the `examples/speech_to_text/run_whisper_speech_to_text.py` file, which demonstrates how to use the Whisper model for speech-to-text tasks.\n\n## Diagram(s)\n```mermaid\nflowchart\n    id1[Audio Input] --> id2[Preprocessing]\n    id2 --> id3[Model Inference]\n    id3 --> id4[Postprocessing]\n    id4 --> id5[Transcript Output]\n```\nThis flowchart shows the overall process of speech-to-text inference, from audio input to transcript output.\n\n## References\n- `examples/speech_to_text/README.md`\n- `examples/speech_to_text/run_whisper_speech_to_text.py`\n- `tests/transformers/test_speech_seq2seq.py`\n- `docs/examples_and_notebooks/speech_to_text.md`"
  },
  "page-21": {
    "title": "QEfficientMPT Notebook",
    "section": "Examples and Notebooks",
    "markdown": "# QEfficientMPT Notebook\n## Overview\nThe QEfficientMPT notebook is designed to provide an efficient and scalable solution for multimodal language models, utilizing the Hugging Face transformers library and supporting both single and dual QPC approaches.\n\n## Key Components / Concepts\nThe QEfficientMPT notebook relies on several key components, including:\n* `QEFFAutoModelForImageTextToText` class, which is used to work with multimodal language models from the Hugging Face hub.\n* `QEFfMptModel` class, which is a modified version of the MptModel class with added support for cache idx and kv retention.\n\n## How it Works\nThe QEfficientMPT notebook works by initializing the `QEFFAutoModelForImageTextToText` class with a pretrained model name or path. It then uses the `from_pretrained` method to load the model and its corresponding configuration.\n\n## Example(s)\nAn example of how to use the QEfficientMPT notebook can be found in the `notebooks/QEfficientMPT.ipynb` file.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Initialize QEFFAutoModelForImageTextToText] --> B[Load Pretrained Model]\n    B --> C[Configure Model]\n    C --> D[Use Model for Inference]\n```\nCaption: QEfficientMPT Notebook Workflow\n\n## References\n* `notebooks/QEfficientMPT.ipynb`\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/transformers/models/mpt/modeling_mpt.py`\n* `README.md`"
  },
  "page-22": {
    "title": "QEfficientGPT2 Notebook",
    "section": "Examples and Notebooks",
    "markdown": "# QEfficientGPT2 Notebook\n## Overview\nThe QEfficientGPT2 notebook is designed to provide an efficient and scalable solution for text generation tasks using the GPT-2 model. This notebook utilizes the QEfficient library, which offers a range of tools and techniques for optimizing the performance of transformer-based models.\n\n## Key Components / Concepts\nThe QEfficientGPT2 notebook relies on several key components, including:\n* The QEFFAutoModelForCausalLM class, which provides a interface for working with causal language models from the HuggingFace hub.\n* The from_pretrained method, which serves as the easiest entry point into using QEfficient and allows for the initialization of models using a similar interface to transformers.AutoModelForCausalLM.\n* The cloud_ai_100_exec_kv function, which generates output until eos or generation_len by executing the compiled qpc on Cloud AI 100 Hardware cards.\n\n## How it Works\nThe QEfficientGPT2 notebook works by first initializing the QEFFAutoModelForCausalLM class using the from_pretrained method. This method takes in several arguments, including the pretrained model name or path, continuous batching, and QAIC config dictionary. Once the model is initialized, the notebook uses the cloud_ai_100_exec_kv function to generate output.\n\n## Example(s)\nTo use the QEfficientGPT2 notebook, simply import the necessary libraries, initialize the QEFFAutoModelForCausalLM class, and call the cloud_ai_100_exec_kv function with the desired arguments.\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Initialize QEFFAutoModelForCausalLM] --> B[Call cloud_ai_100_exec_kv]\n    B --> C[Generate Output]\n    C --> D[Return Results]\n```\nCaption: High-level overview of the QEfficientGPT2 notebook workflow.\n\n## References\n* `QEfficient/transformers/models/modeling_auto.py`\n* `QEfficient/generation/text_generation_inference.py`\n* `notebooks/QEfficientGPT2.ipynb`\n* `QEfficient/utils/modeling_utils.py`"
  },
  "page-26": {
    "title": "Getting Started with Efficient Transformers",
    "section": "Examples and Notebooks",
    "markdown": "# Getting Started with Efficient Transformers\n## Overview\nThe Efficient Transformers library is designed to optimize transformer models for Cloud AI 100. This guide provides a step-by-step introduction to getting started with the library, located in the `QEfficient` directory.\n\n## Key Components / Concepts\nThe library provides several key components, including:\n* `QEFFAutoModel`: a class for manipulating any transformer model from the HuggingFace hub, defined in [QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)\n* `QEFFTransformersBase`: a parent class for models provided by the library, outlined in [QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)\n* `QEFFBaseModel`: a base class for all model classes, described in [docs/source/quick_start.md](docs/source/quick_start.md)\n\n## How it Works\nTo use the library, you can initialize a model using the `from_pretrained` method, similar to `transformers.AutoModel`. You can then compile the model for Cloud AI 100 using the `compile` method, as detailed in [docs/README.md](docs/README.md).\n\n## Example(s)\nHere is an example of how to initialize and compile a model:\n```python\nfrom QEfficient import QEFFAutoModel\nfrom transformers import AutoTokenizer\n\n# Initialize the model using from_pretrained\nmodel = QEFFAutoModel.from_pretrained(\"model_name\")\n\n# Compile the model for Cloud AI 100\nmodel.compile(num_cores=16)\n```\n\n## Diagram(s)\n```mermaid\nflowchart\n    A[Initialize Model] --> B[Compile Model]\n    B --> C[Optimize Model]\n    C --> D[Deploy Model]\n```\nThis flowchart illustrates the basic steps involved in using the Efficient Transformers library.\n\n## References\n* `[QEfficient/transformers/models/modeling_auto.py](QEfficient/transformers/models/modeling_auto.py)`\n* `[QEfficient/transformers/transform.py](QEfficient/transformers/transform.py)`\n* `[docs/README.md](docs/README.md)`\n* `[docs/source/quick_start.md](docs/source/quick_start.md)`"
  }
}