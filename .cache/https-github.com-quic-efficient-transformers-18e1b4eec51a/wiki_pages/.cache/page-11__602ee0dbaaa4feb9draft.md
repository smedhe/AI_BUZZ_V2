# Speculative Decoding Demonstrations
## Overview
Speculative decoding is a technique used in natural language processing to improve the efficiency of language models. It involves generating multiple possible outputs for a given input and then selecting the most likely one. This technique can be particularly useful for applications such as language translation, text summarization, and chatbots. In this section, we will explore the speculative decoding demonstrations available in the repository, including multi-project speculative decoding, draft-based decoding, and PLD decoding on Qualcomm Cloud AI 100.

## Key Components / Concepts
The key components of speculative decoding include:
* **Speculative tokens**: These are the possible output tokens generated by the model for a given input.
* **Prefill sequence length**: This is the length of the input sequence used to generate the speculative tokens.
* **Context length**: This is the length of the context used to generate the speculative tokens.
* **Prefill batch size**: This is the batch size used to generate the speculative tokens.
* **Target model**: This is the model used to generate the final output.
* **Draft model**: This is the model used to generate the speculative tokens.

## How it Works
The speculative decoding process works as follows:
1. The input prompt is tokenized and passed to the draft model to generate speculative tokens.
2. The speculative tokens are then passed to the target model to generate the final output.
3. The final output is selected based on the likelihood of each possible output.

## Example(s)
Here is an example of how to use the `draft_spec_decode_inference` function to perform speculative decoding:
```python
from examples.draft_spd_inference import draft_spec_decode_inference

prompts = ["Hello, how are you?"]
num_speculative_tokens = 10
prefill_seq_len = 32
ctx_len = 128
prefill_bsz = 1
draft_model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
target_model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
full_batch_size = None
target_device_group = [0]
draft_device_group = [0]

output = draft_spec_decode_inference(
    prompts,
    num_speculative_tokens,
    prefill_seq_len,
    ctx_len,
    prefill_bsz,
    draft_model_name,
    target_model_name,
    full_batch_size,
    target_device_group,
    draft_device_group,
)
```
## Diagram(s)
```mermaid
flowchart LR
    A[Input Prompt] -->|Tokenize|> B[Tokenized Prompt]
    B -->|Pass to Draft Model|> C[Speculative Tokens]
    C -->|Pass to Target Model|> D[Final Output]
    D -->|Select Final Output|> E[Output]
```
This flowchart shows the speculative decoding process, from input prompt to final output.

## References
* `examples/draft_spd_inference.py`: This file contains the `draft_spec_decode_inference` function, which performs speculative decoding using a draft model and a target model.
* `examples/multiprojs_spd_inference.py`: This file contains the `multiprojs_spec_decode_inference` function, which performs speculative decoding using multiple projects.
* `examples/pld_spd_inference.py`: This file contains the `pld_spec_decode_inference` function, which performs speculative decoding using a PLD model.
* `tests/transformers/spd/test_pld_inference.py`: This file contains tests for the `pld_spec_decode_inference` function.