# Visionâ€‘Language Model Inference Examples
## Overview
Vision-language models are a type of multimodal model that can process and generate text based on visual inputs, such as images. These models have a wide range of applications, including image captioning, visual question answering, and text-to-image synthesis. In this section, we will provide examples of how to use vision-language models for inference, including code snippets and explanations of the key components and concepts involved.

## Key Components / Concepts
The key components of a vision-language model include:

* **Vision encoder**: This is the component of the model that processes the visual input, such as an image, and extracts features from it.
* **Language decoder**: This is the component of the model that generates text based on the features extracted by the vision encoder.
* **Multimodal fusion**: This refers to the process of combining the features extracted by the vision encoder with the text generated by the language decoder to produce a final output.

Some key concepts involved in vision-language models include:

* **Multimodal learning**: This refers to the process of learning from multiple sources of data, such as images and text.
* **Transfer learning**: This refers to the process of using a pre-trained model as a starting point for a new task, rather than training a model from scratch.

## How it Works
The process of using a vision-language model for inference typically involves the following steps:

1. **Preprocessing**: The input image is preprocessed to extract features from it, such as resizing and normalizing the image.
2. **Vision encoding**: The preprocessed image is passed through the vision encoder to extract features from it.
3. **Language decoding**: The features extracted by the vision encoder are passed through the language decoder to generate text.
4. **Multimodal fusion**: The features extracted by the vision encoder are combined with the text generated by the language decoder to produce a final output.

## Example(s)
Here is an example of how to use the InternVL model for inference:
```python
from QEfficient import QEFFAutoModelForCausalLM
from transformers import AutoTokenizer, AutoProcessor

# Load the model and tokenizer
model = QEFFAutoModelForCausalLM.from_pretrained("internvl-1b")
tokenizer = AutoTokenizer.from_pretrained("internvl-1b")
processor = AutoProcessor.from_pretrained("internvl-1b")

# Preprocess the input image
image = Image.open("image.jpg")
image = image.resize((256, 256))

# Pass the image through the vision encoder
vision_features = model.vision_encoder(image)

# Generate text based on the vision features
text = model.generate(vision_features, max_length=100)

# Print the generated text
print(text)
```
## Diagram(s)
```mermaid
graph LR
    A[Input Image] -->|Preprocessing|> B[Preprocessed Image]
    B -->|Vision Encoding|> C[Vision Features]
    C -->|Language Decoding|> D[Generated Text]
    D -->|Multimodal Fusion|> E[Final Output]
```
This diagram shows the process of using a vision-language model for inference, including preprocessing, vision encoding, language decoding, and multimodal fusion.

## References
* `QEfficient/cloud/infer.py`: This file contains the `execute_vlm_model` function, which generates output from a compiled Vision-Language Model (VLM) on Cloud AI 100 hardware.
* `examples/intern_example/internvl_inference.py`: This file contains an example script for running inference on the InternVL-1B model using the QEFFAutoModelForCausalLM class.
* `examples/granite_example/granite_vision_inference.py`: This file contains an example script for running inference on the Granite-vision-3.2-2b model using the QEFFAutoModelForCausalLM class.
* `examples/llama4_example.py`: This file contains an example script for running inference on the Llama-4 model using the QEFFAutoModelForImageTextToText class.